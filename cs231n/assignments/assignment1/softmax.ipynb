{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"softmax.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"wqIqnRjx00cp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":451},"executionInfo":{"status":"ok","timestamp":1595475826893,"user_tz":240,"elapsed":7628,"user":{"displayName":"Emerson Li","photoUrl":"","userId":"16602955780450158924"}},"outputId":"6f06eeb4-4d2c-49e2-9e5d-8a8c2468e7ec"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# 'cs231n' folder containing the '.py', 'classifiers' and 'datasets'\n","# folders.\n","# e.g. 'cs231n/assignments/assignment1/cs231n/'\n","FOLDERNAME = 'cs231n/assignments/assignment1/cs231n/'\n","\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","%cd drive/My\\ Drive\n","%cp -r $FOLDERNAME ../../\n","%cd ../../\n","%cd cs231n/datasets/\n","!bash get_datasets.sh\n","%cd ../../"],"execution_count":71,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive\n","/content\n","/content/cs231n/datasets\n","--2020-07-23 03:43:41--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170498071 (163M) [application/x-gzip]\n","Saving to: ‘cifar-10-python.tar.gz’\n","\n","cifar-10-python.tar 100%[===================>] 162.60M  55.5MB/s    in 2.9s    \n","\n","2020-07-23 03:43:44 (55.5 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n","\n","cifar-10-batches-py/\n","cifar-10-batches-py/data_batch_4\n","cifar-10-batches-py/readme.html\n","cifar-10-batches-py/test_batch\n","cifar-10-batches-py/data_batch_3\n","cifar-10-batches-py/batches.meta\n","cifar-10-batches-py/data_batch_2\n","cifar-10-batches-py/data_batch_5\n","cifar-10-batches-py/data_batch_1\n","/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"tags":["pdf-title"],"id":"nDaJfWme00cs","colab_type":"text"},"source":["# Softmax exercise\n","\n","*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n","\n","This exercise is analogous to the SVM exercise. You will:\n","\n","- implement a fully-vectorized **loss function** for the Softmax classifier\n","- implement the fully-vectorized expression for its **analytic gradient**\n","- **check your implementation** with numerical gradient\n","- use a validation set to **tune the learning rate and regularization** strength\n","- **optimize** the loss function with **SGD**\n","- **visualize** the final learned weights\n"]},{"cell_type":"code","metadata":{"tags":["pdf-ignore"],"id":"nsD0j--t00cs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1595475830025,"user_tz":240,"elapsed":145,"user":{"displayName":"Emerson Li","photoUrl":"","userId":"16602955780450158924"}},"outputId":"eba1227c-dc81-4446-c634-008f2f983b2e"},"source":["import random\n","import numpy as np\n","from cs231n.data_utils import load_CIFAR10\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading extenrnal modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":72,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"tags":["pdf-ignore"],"id":"HKbGpH0300cu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1595475834623,"user_tz":240,"elapsed":2416,"user":{"displayName":"Emerson Li","photoUrl":"","userId":"16602955780450158924"}},"outputId":"f3a5ae85-f18a-43c2-d5ba-768800ea8b17"},"source":["def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n","    \"\"\"\n","    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n","    it for the linear classifier. These are the same steps as we used for the\n","    SVM, but condensed to a single function.  \n","    \"\"\"\n","    # Load the raw CIFAR-10 data\n","    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n","    \n","    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n","    try:\n","       del X_train, y_train\n","       del X_test, y_test\n","       print('Clear previously loaded data.')\n","    except:\n","       pass\n","\n","    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n","    \n","    # subsample the data\n","    mask = list(range(num_training, num_training + num_validation))\n","    X_val = X_train[mask]\n","    y_val = y_train[mask]\n","    mask = list(range(num_training))\n","    X_train = X_train[mask]\n","    y_train = y_train[mask]\n","    mask = list(range(num_test))\n","    X_test = X_test[mask]\n","    y_test = y_test[mask]\n","    mask = np.random.choice(num_training, num_dev, replace=False)\n","    X_dev = X_train[mask]\n","    y_dev = y_train[mask]\n","    \n","    # Preprocessing: reshape the image data into rows\n","    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n","    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n","    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n","    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n","    \n","    # Normalize the data: subtract the mean image\n","    mean_image = np.mean(X_train, axis = 0)\n","    X_train -= mean_image\n","    X_val -= mean_image\n","    X_test -= mean_image\n","    X_dev -= mean_image\n","    \n","    # add bias dimension and transform into columns\n","    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n","    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n","    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n","    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n","    \n","    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n","\n","\n","# Invoke the above function to get our data.\n","X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n","print('Train data shape: ', X_train.shape)\n","print('Train labels shape: ', y_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Validation labels shape: ', y_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)\n","print('dev data shape: ', X_dev.shape)\n","print('dev labels shape: ', y_dev.shape)"],"execution_count":73,"outputs":[{"output_type":"stream","text":["Train data shape:  (49000, 3073)\n","Train labels shape:  (49000,)\n","Validation data shape:  (1000, 3073)\n","Validation labels shape:  (1000,)\n","Test data shape:  (1000, 3073)\n","Test labels shape:  (1000,)\n","dev data shape:  (500, 3073)\n","dev labels shape:  (500,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ytTPZVqa00cw","colab_type":"text"},"source":["## Softmax Classifier\n","\n","Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"]},{"cell_type":"code","metadata":{"id":"wcbzeGwJ00cx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1595475839428,"user_tz":240,"elapsed":355,"user":{"displayName":"Emerson Li","photoUrl":"","userId":"16602955780450158924"}},"outputId":"e62ec759-52e6-49c3-d02c-b654494847d9"},"source":["# First implement the naive softmax loss function with nested loops.\n","# Open the file cs231n/classifiers/softmax.py and implement the\n","# softmax_loss_naive function.\n","\n","from cs231n.classifiers.softmax import softmax_loss_naive\n","import time\n","\n","# Generate a random softmax weight matrix and use it to compute the loss.\n","W = np.random.randn(3073, 10) * 0.0001\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n","\n","# As a rough sanity check, our loss should be something close to -log(0.1).\n","print('loss: %f' % loss)\n","print('sanity check: %f' % (-np.log(0.1)))"],"execution_count":74,"outputs":[{"output_type":"stream","text":["loss: 2.376793\n","sanity check: 2.302585\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"tags":["pdf-inline"],"id":"g05ovERj00cz","colab_type":"text"},"source":["**Inline Question 1**\n","\n","Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n","\n","$\\color{blue}{\\textit Your Answer:}$ *Because there are 10 classes with 1 correct class, if assume the scores for each are similar, then loss should be close to -log(1/10)* \n","\n"]},{"cell_type":"code","metadata":{"id":"SQJg9zr7iQ6w","colab_type":"code","colab":{}},"source":["import inspect"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQeaxKykiWO3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":953},"executionInfo":{"status":"ok","timestamp":1595455175277,"user_tz":240,"elapsed":152,"user":{"displayName":"Emerson Li","photoUrl":"","userId":"16602955780450158924"}},"outputId":"81e74d00-0ead-4a11-f940-d9df0326e3a0"},"source":["print(inspect.getsource(softmax_loss_naive))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["def softmax_loss_naive(W, X, y, reg):\n","    \"\"\"\n","    Softmax loss function, naive implementation (with loops)\n","\n","    Inputs have dimension D, there are C classes, and we operate on minibatches\n","    of N examples.\n","\n","    Inputs:\n","    - W: A numpy array of shape (D, C) containing weights.\n","    - X: A numpy array of shape (N, D) containing a minibatch of data.\n","    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n","      that X[i] has label c, where 0 <= c < C.\n","    - reg: (float) regularization strength\n","\n","    Returns a tuple of:\n","    - loss as single float\n","    - gradient with respect to weights W; an array of same shape as W\n","    \"\"\"\n","    # Initialize the loss and gradient to zero.\n","    loss = 0.0\n","    dW = np.zeros_like(W)\n","\n","    #############################################################################\n","    # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n","    # Store the loss in loss and the gradient in dW. If you are not careful     #\n","    # here, it is easy to run into numeric instability. Don't forget the        #\n","    # regularization!                                                           #\n","    #############################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    scores = np.dot(X, W)\n","    num_samples = scores.shape[0]\n","    num_classes = scores.shape[1]\n","    sample_max = np.amax(scores, axis=1)\n","    stable_scores = scores - sample_max[:, np.newaxis]\n","    for i in range(num_samples):\n","      num = np.exp(stable_scores[i, y[i]])\n","      den = np.sum(np.exp(stable_scores[i]))\n","      c = np.exp(-sample_max[i])\n","      for j in range(num_classes):\n","        if j == y[i]:\n","          dW[:, j] -= X[i] * den / (den - np.exp(stable_scores[i, j]))\n","        else:\n","          dW[:, j] -= c * X[i] * np.exp(stable_scores[i, j]) * den / num\n","      loss += - np.log(num / den)\n","\n","    dW /= num_samples\n","    dW += 2 * reg * W\n","    loss /= num_samples\n","    loss += reg * np.sum(W * W)\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    return loss, dW\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V_JNOWVz00cz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":364},"executionInfo":{"status":"ok","timestamp":1595471015962,"user_tz":240,"elapsed":5862,"user":{"displayName":"Emerson Li","photoUrl":"","userId":"16602955780450158924"}},"outputId":"2eaff634-6fde-4901-f92a-bf0425ba2013"},"source":["# Complete the implementation of softmax_loss_naive and implement a (naive)\n","# version of the gradient that uses nested loops.\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n","\n","# As we did for the SVM, use numeric gradient checking as a debugging tool.\n","# The numeric gradient should be close to the analytic gradient.\n","from cs231n.gradient_check import grad_check_sparse\n","f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n","grad_numerical = grad_check_sparse(f, W, grad, 10)\n","\n","# similar to SVM case, do another gradient check with regularization\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n","f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n","grad_numerical = grad_check_sparse(f, W, grad, 10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["numerical: -0.884191 analytic: -0.884191, relative error: 4.626946e-09\n","numerical: 0.374728 analytic: 0.374728, relative error: 1.360251e-07\n","numerical: -0.615089 analytic: -0.615089, relative error: 7.144104e-08\n","numerical: 1.291675 analytic: 1.291675, relative error: 6.800898e-09\n","numerical: 0.158810 analytic: 0.158810, relative error: 3.812892e-07\n","numerical: -0.409329 analytic: -0.409329, relative error: 2.313595e-09\n","numerical: -1.795070 analytic: -1.795070, relative error: 1.199665e-08\n","numerical: 1.628998 analytic: 1.628998, relative error: 3.105058e-09\n","numerical: -0.716456 analytic: -0.716456, relative error: 4.869689e-08\n","numerical: -4.142816 analytic: -4.142816, relative error: 7.082437e-09\n","numerical: 2.396051 analytic: 2.396051, relative error: 2.134917e-08\n","numerical: 3.214124 analytic: 3.214124, relative error: 1.072131e-08\n","numerical: -0.797871 analytic: -0.797871, relative error: 3.310812e-08\n","numerical: 0.590280 analytic: 0.590280, relative error: 5.938728e-08\n","numerical: 0.263089 analytic: 0.263089, relative error: 1.557152e-07\n","numerical: -0.780142 analytic: -0.780142, relative error: 1.011292e-08\n","numerical: -0.733286 analytic: -0.733286, relative error: 6.482472e-09\n","numerical: -3.202727 analytic: -3.202727, relative error: 1.373076e-09\n","numerical: -3.649957 analytic: -3.649957, relative error: 5.392982e-09\n","numerical: -0.235247 analytic: -0.235247, relative error: 2.033999e-07\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r1VBjAVs00c1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1595475845050,"user_tz":240,"elapsed":320,"user":{"displayName":"Emerson Li","photoUrl":"","userId":"16602955780450158924"}},"outputId":"fe539269-317d-41ba-d4fc-4989d7140372"},"source":["# Now that we have a naive implementation of the softmax loss function and its gradient,\n","# implement a vectorized version in softmax_loss_vectorized.\n","# The two versions should compute the same results, but the vectorized version should be\n","# much faster.\n","tic = time.time()\n","loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n","toc = time.time()\n","print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n","\n","from cs231n.classifiers.softmax import softmax_loss_vectorized\n","tic = time.time()\n","loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n","toc = time.time()\n","print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n","\n","# As we did for the SVM, we use the Frobenius norm to compare the two versions\n","# of the gradient.\n","grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n","print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n","print('Gradient difference: %f' % grad_difference)"],"execution_count":75,"outputs":[{"output_type":"stream","text":["naive loss: 2.376793e+00 computed in 0.132212s\n","vectorized loss: 2.376793e+00 computed in 0.014305s\n","Loss difference: 0.000000\n","Gradient difference: 0.000000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tuning","tags":["code"],"colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595477908967,"user_tz":240,"elapsed":807705,"user":{"displayName":"Emerson Li","photoUrl":"","userId":"16602955780450158924"}},"outputId":"e9bb8bab-9d94-4aeb-fae4-11241426bff9"},"source":["# Use the validation set to tune hyperparameters (regularization strength and\n","# learning rate). You should experiment with different ranges for the learning\n","# rates and regularization strengths; if you are careful you should be able to\n","# get a classification accuracy of over 0.35 on the validation set.\n","\n","from cs231n.classifiers import Softmax\n","results = {}\n","best_val = -1\n","best_softmax = None\n","\n","################################################################################\n","# TODO:                                                                        #\n","# Use the validation set to set the learning rate and regularization strength. #\n","# This should be identical to the validation that you did for the SVM; save    #\n","# the best trained softmax classifer in best_softmax.                          #\n","################################################################################\n","\n","# Provided as a reference. You may or may not want to change these hyperparameters\n","learning_rates = np.linspace(1e-7, 5e-7, num=10)\n","regularization_strengths = np.linspace(2.5e4, 5e4, num=10)\n","\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","best = []\n","for lr in learning_rates:\n","  for rs in regularization_strengths:\n","    softmax = Softmax()\n","    loss_hist = softmax.train(X_train, y_train, learning_rate=lr,\n","                          reg=rs, num_iters=1500, verbose=True)\n","    y_train_pred = softmax.predict(X_train)\n","    y_val_pred = softmax.predict(X_val)\n","    train_accuracy = np.mean((y_train == y_train_pred))\n","    val_accuracy = np.mean((y_val == y_val_pred))\n","    best.append((softmax, val_accuracy))\n","    results[(lr, rs)] = (train_accuracy, val_accuracy)\n","    \n","(best_softmax, best_val) = max(best, key=lambda item:item[1])\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    \n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n","                lr, reg, train_accuracy, val_accuracy))\n","    \n","print('best validation accuracy achieved during cross-validation: %f' % best_val)"],"execution_count":83,"outputs":[{"output_type":"stream","text":["iteration 0 / 1500: loss 761.398925\n","iteration 100 / 1500: loss 279.329306\n","iteration 200 / 1500: loss 103.502718\n","iteration 300 / 1500: loss 39.199680\n","iteration 400 / 1500: loss 15.674741\n","iteration 500 / 1500: loss 7.089787\n","iteration 600 / 1500: loss 3.919407\n","iteration 700 / 1500: loss 2.747882\n","iteration 800 / 1500: loss 2.336475\n","iteration 900 / 1500: loss 2.190976\n","iteration 1000 / 1500: loss 2.079152\n","iteration 1100 / 1500: loss 2.113833\n","iteration 1200 / 1500: loss 2.069591\n","iteration 1300 / 1500: loss 2.102290\n","iteration 1400 / 1500: loss 2.092815\n","iteration 0 / 1500: loss 861.633386\n","iteration 100 / 1500: loss 282.622920\n","iteration 200 / 1500: loss 93.950948\n","iteration 300 / 1500: loss 32.213414\n","iteration 400 / 1500: loss 11.931755\n","iteration 500 / 1500: loss 5.382992\n","iteration 600 / 1500: loss 3.159937\n","iteration 700 / 1500: loss 2.427492\n","iteration 800 / 1500: loss 2.214565\n","iteration 900 / 1500: loss 2.089556\n","iteration 1000 / 1500: loss 2.054864\n","iteration 1100 / 1500: loss 2.105858\n","iteration 1200 / 1500: loss 2.077546\n","iteration 1300 / 1500: loss 2.061409\n","iteration 1400 / 1500: loss 2.120018\n","iteration 0 / 1500: loss 948.053660\n","iteration 100 / 1500: loss 278.702592\n","iteration 200 / 1500: loss 83.042717\n","iteration 300 / 1500: loss 25.748509\n","iteration 400 / 1500: loss 9.040776\n","iteration 500 / 1500: loss 4.149239\n","iteration 600 / 1500: loss 2.696908\n","iteration 700 / 1500: loss 2.301973\n","iteration 800 / 1500: loss 2.153851\n","iteration 900 / 1500: loss 2.174419\n","iteration 1000 / 1500: loss 2.105541\n","iteration 1100 / 1500: loss 2.098468\n","iteration 1200 / 1500: loss 2.160592\n","iteration 1300 / 1500: loss 2.121946\n","iteration 1400 / 1500: loss 2.119254\n","iteration 0 / 1500: loss 1026.652347\n","iteration 100 / 1500: loss 269.969598\n","iteration 200 / 1500: loss 72.142429\n","iteration 300 / 1500: loss 20.437517\n","iteration 400 / 1500: loss 6.961223\n","iteration 500 / 1500: loss 3.365236\n","iteration 600 / 1500: loss 2.442878\n","iteration 700 / 1500: loss 2.142722\n","iteration 800 / 1500: loss 2.109208\n","iteration 900 / 1500: loss 2.127929\n","iteration 1000 / 1500: loss 2.127942\n","iteration 1100 / 1500: loss 2.097943\n","iteration 1200 / 1500: loss 2.155994\n","iteration 1300 / 1500: loss 2.075130\n","iteration 1400 / 1500: loss 2.086438\n","iteration 0 / 1500: loss 1104.630207\n","iteration 100 / 1500: loss 259.539464\n","iteration 200 / 1500: loss 62.178071\n","iteration 300 / 1500: loss 16.198302\n","iteration 400 / 1500: loss 5.396401\n","iteration 500 / 1500: loss 2.890059\n","iteration 600 / 1500: loss 2.302004\n","iteration 700 / 1500: loss 2.192337\n","iteration 800 / 1500: loss 2.135760\n","iteration 900 / 1500: loss 2.064804\n","iteration 1000 / 1500: loss 2.121074\n","iteration 1100 / 1500: loss 2.095351\n","iteration 1200 / 1500: loss 2.135582\n","iteration 1300 / 1500: loss 2.130178\n","iteration 1400 / 1500: loss 2.128771\n","iteration 0 / 1500: loss 1203.233262\n","iteration 100 / 1500: loss 252.988010\n","iteration 200 / 1500: loss 54.555135\n","iteration 300 / 1500: loss 13.131022\n","iteration 400 / 1500: loss 4.382823\n","iteration 500 / 1500: loss 2.642668\n","iteration 600 / 1500: loss 2.206425\n","iteration 700 / 1500: loss 2.116524\n","iteration 800 / 1500: loss 2.170814\n","iteration 900 / 1500: loss 2.055894\n","iteration 1000 / 1500: loss 2.154627\n","iteration 1100 / 1500: loss 2.110998\n","iteration 1200 / 1500: loss 2.117618\n","iteration 1300 / 1500: loss 2.160786\n","iteration 1400 / 1500: loss 2.083270\n","iteration 0 / 1500: loss 1268.183029\n","iteration 100 / 1500: loss 238.661038\n","iteration 200 / 1500: loss 46.414545\n","iteration 300 / 1500: loss 10.384061\n","iteration 400 / 1500: loss 3.715284\n","iteration 500 / 1500: loss 2.441789\n","iteration 600 / 1500: loss 2.176851\n","iteration 700 / 1500: loss 2.160628\n","iteration 800 / 1500: loss 2.143223\n","iteration 900 / 1500: loss 2.113167\n","iteration 1000 / 1500: loss 2.164447\n","iteration 1100 / 1500: loss 2.122933\n","iteration 1200 / 1500: loss 2.072593\n","iteration 1300 / 1500: loss 2.162958\n","iteration 1400 / 1500: loss 2.116372\n","iteration 0 / 1500: loss 1378.134552\n","iteration 100 / 1500: loss 232.102298\n","iteration 200 / 1500: loss 40.552535\n","iteration 300 / 1500: loss 8.609302\n","iteration 400 / 1500: loss 3.216477\n","iteration 500 / 1500: loss 2.271717\n","iteration 600 / 1500: loss 2.130015\n","iteration 700 / 1500: loss 2.141660\n","iteration 800 / 1500: loss 2.146868\n","iteration 900 / 1500: loss 2.157857\n","iteration 1000 / 1500: loss 2.071973\n","iteration 1100 / 1500: loss 2.146917\n","iteration 1200 / 1500: loss 2.186180\n","iteration 1300 / 1500: loss 2.138443\n","iteration 1400 / 1500: loss 2.164866\n","iteration 0 / 1500: loss 1450.943243\n","iteration 100 / 1500: loss 218.255745\n","iteration 200 / 1500: loss 34.467359\n","iteration 300 / 1500: loss 6.948197\n","iteration 400 / 1500: loss 2.815606\n","iteration 500 / 1500: loss 2.262585\n","iteration 600 / 1500: loss 2.166296\n","iteration 700 / 1500: loss 2.105095\n","iteration 800 / 1500: loss 2.147878\n","iteration 900 / 1500: loss 2.122970\n","iteration 1000 / 1500: loss 2.141250\n","iteration 1100 / 1500: loss 2.096215\n","iteration 1200 / 1500: loss 2.145233\n","iteration 1300 / 1500: loss 2.208955\n","iteration 1400 / 1500: loss 2.131074\n","iteration 0 / 1500: loss 1543.180051\n","iteration 100 / 1500: loss 207.652710\n","iteration 200 / 1500: loss 29.595565\n","iteration 300 / 1500: loss 5.822801\n","iteration 400 / 1500: loss 2.629675\n","iteration 500 / 1500: loss 2.236979\n","iteration 600 / 1500: loss 2.172746\n","iteration 700 / 1500: loss 2.129116\n","iteration 800 / 1500: loss 2.189995\n","iteration 900 / 1500: loss 2.152734\n","iteration 1000 / 1500: loss 2.124566\n","iteration 1100 / 1500: loss 2.112107\n","iteration 1200 / 1500: loss 2.146442\n","iteration 1300 / 1500: loss 2.171011\n","iteration 1400 / 1500: loss 2.115898\n","iteration 0 / 1500: loss 760.400757\n","iteration 100 / 1500: loss 179.112873\n","iteration 200 / 1500: loss 43.401920\n","iteration 300 / 1500: loss 11.733118\n","iteration 400 / 1500: loss 4.297239\n","iteration 500 / 1500: loss 2.613380\n","iteration 600 / 1500: loss 2.203998\n","iteration 700 / 1500: loss 2.085418\n","iteration 800 / 1500: loss 2.131522\n","iteration 900 / 1500: loss 2.019637\n","iteration 1000 / 1500: loss 2.123111\n","iteration 1100 / 1500: loss 2.066610\n","iteration 1200 / 1500: loss 2.090522\n","iteration 1300 / 1500: loss 2.046756\n","iteration 1400 / 1500: loss 2.107500\n","iteration 0 / 1500: loss 865.058277\n","iteration 100 / 1500: loss 173.558545\n","iteration 200 / 1500: loss 36.210794\n","iteration 300 / 1500: loss 8.919841\n","iteration 400 / 1500: loss 3.393831\n","iteration 500 / 1500: loss 2.381605\n","iteration 600 / 1500: loss 2.152771\n","iteration 700 / 1500: loss 2.154021\n","iteration 800 / 1500: loss 2.086904\n","iteration 900 / 1500: loss 2.112272\n","iteration 1000 / 1500: loss 2.113545\n","iteration 1100 / 1500: loss 2.085947\n","iteration 1200 / 1500: loss 2.098677\n","iteration 1300 / 1500: loss 2.103594\n","iteration 1400 / 1500: loss 2.109123\n","iteration 0 / 1500: loss 944.025665\n","iteration 100 / 1500: loss 161.103455\n","iteration 200 / 1500: loss 29.015475\n","iteration 300 / 1500: loss 6.595209\n","iteration 400 / 1500: loss 2.935276\n","iteration 500 / 1500: loss 2.233355\n","iteration 600 / 1500: loss 2.098134\n","iteration 700 / 1500: loss 2.158300\n","iteration 800 / 1500: loss 2.062128\n","iteration 900 / 1500: loss 2.078897\n","iteration 1000 / 1500: loss 2.112723\n","iteration 1100 / 1500: loss 2.162849\n","iteration 1200 / 1500: loss 2.123101\n","iteration 1300 / 1500: loss 2.135150\n","iteration 1400 / 1500: loss 2.076190\n","iteration 0 / 1500: loss 1034.846443\n","iteration 100 / 1500: loss 150.369799\n","iteration 200 / 1500: loss 23.514025\n","iteration 300 / 1500: loss 5.180739\n","iteration 400 / 1500: loss 2.546604\n","iteration 500 / 1500: loss 2.155820\n","iteration 600 / 1500: loss 2.081779\n","iteration 700 / 1500: loss 2.130782\n","iteration 800 / 1500: loss 2.109895\n","iteration 900 / 1500: loss 2.107329\n","iteration 1000 / 1500: loss 2.138141\n","iteration 1100 / 1500: loss 2.085037\n","iteration 1200 / 1500: loss 2.144813\n","iteration 1300 / 1500: loss 2.104264\n","iteration 1400 / 1500: loss 2.130641\n","iteration 0 / 1500: loss 1098.056370\n","iteration 100 / 1500: loss 136.034149\n","iteration 200 / 1500: loss 18.535110\n","iteration 300 / 1500: loss 4.152051\n","iteration 400 / 1500: loss 2.363231\n","iteration 500 / 1500: loss 2.200195\n","iteration 600 / 1500: loss 2.142841\n","iteration 700 / 1500: loss 2.070544\n","iteration 800 / 1500: loss 2.113858\n","iteration 900 / 1500: loss 2.087561\n","iteration 1000 / 1500: loss 2.121731\n","iteration 1100 / 1500: loss 2.097230\n","iteration 1200 / 1500: loss 2.128909\n","iteration 1300 / 1500: loss 2.138319\n","iteration 1400 / 1500: loss 2.100719\n","iteration 0 / 1500: loss 1201.680822\n","iteration 100 / 1500: loss 126.661883\n","iteration 200 / 1500: loss 15.019528\n","iteration 300 / 1500: loss 3.467107\n","iteration 400 / 1500: loss 2.346087\n","iteration 500 / 1500: loss 2.118199\n","iteration 600 / 1500: loss 2.111999\n","iteration 700 / 1500: loss 2.092536\n","iteration 800 / 1500: loss 2.127005\n","iteration 900 / 1500: loss 2.192328\n","iteration 1000 / 1500: loss 2.119880\n","iteration 1100 / 1500: loss 2.110800\n","iteration 1200 / 1500: loss 2.156122\n","iteration 1300 / 1500: loss 2.121304\n","iteration 1400 / 1500: loss 2.115380\n","iteration 0 / 1500: loss 1296.599417\n","iteration 100 / 1500: loss 116.372547\n","iteration 200 / 1500: loss 12.218324\n","iteration 300 / 1500: loss 3.019768\n","iteration 400 / 1500: loss 2.201177\n","iteration 500 / 1500: loss 2.166565\n","iteration 600 / 1500: loss 2.168498\n","iteration 700 / 1500: loss 2.114937\n","iteration 800 / 1500: loss 2.111867\n","iteration 900 / 1500: loss 2.126196\n","iteration 1000 / 1500: loss 2.179543\n","iteration 1100 / 1500: loss 2.149673\n","iteration 1200 / 1500: loss 2.206510\n","iteration 1300 / 1500: loss 2.190388\n","iteration 1400 / 1500: loss 2.119927\n","iteration 0 / 1500: loss 1376.789832\n","iteration 100 / 1500: loss 105.151845\n","iteration 200 / 1500: loss 9.914806\n","iteration 300 / 1500: loss 2.754330\n","iteration 400 / 1500: loss 2.158229\n","iteration 500 / 1500: loss 2.147789\n","iteration 600 / 1500: loss 2.128355\n","iteration 700 / 1500: loss 2.146086\n","iteration 800 / 1500: loss 2.101812\n","iteration 900 / 1500: loss 2.183311\n","iteration 1000 / 1500: loss 2.159154\n","iteration 1100 / 1500: loss 2.068720\n","iteration 1200 / 1500: loss 2.103626\n","iteration 1300 / 1500: loss 2.133012\n","iteration 1400 / 1500: loss 2.073836\n","iteration 0 / 1500: loss 1452.450980\n","iteration 100 / 1500: loss 94.553862\n","iteration 200 / 1500: loss 8.011110\n","iteration 300 / 1500: loss 2.525324\n","iteration 400 / 1500: loss 2.179607\n","iteration 500 / 1500: loss 2.158499\n","iteration 600 / 1500: loss 2.116744\n","iteration 700 / 1500: loss 2.141280\n","iteration 800 / 1500: loss 2.109188\n","iteration 900 / 1500: loss 2.143496\n","iteration 1000 / 1500: loss 2.094333\n","iteration 1100 / 1500: loss 2.133150\n","iteration 1200 / 1500: loss 2.158116\n","iteration 1300 / 1500: loss 2.128513\n","iteration 1400 / 1500: loss 2.160511\n","iteration 0 / 1500: loss 1542.887805\n","iteration 100 / 1500: loss 85.758512\n","iteration 200 / 1500: loss 6.691003\n","iteration 300 / 1500: loss 2.386584\n","iteration 400 / 1500: loss 2.122730\n","iteration 500 / 1500: loss 2.157801\n","iteration 600 / 1500: loss 2.165359\n","iteration 700 / 1500: loss 2.195996\n","iteration 800 / 1500: loss 2.081601\n","iteration 900 / 1500: loss 2.128300\n","iteration 1000 / 1500: loss 2.168234\n","iteration 1100 / 1500: loss 2.114284\n","iteration 1200 / 1500: loss 2.102241\n","iteration 1300 / 1500: loss 2.076224\n","iteration 1400 / 1500: loss 2.151907\n","iteration 0 / 1500: loss 764.846058\n","iteration 100 / 1500: loss 115.787753\n","iteration 200 / 1500: loss 19.026664\n","iteration 300 / 1500: loss 4.625430\n","iteration 400 / 1500: loss 2.465609\n","iteration 500 / 1500: loss 2.190574\n","iteration 600 / 1500: loss 2.086171\n","iteration 700 / 1500: loss 2.141628\n","iteration 800 / 1500: loss 2.082712\n","iteration 900 / 1500: loss 2.033429\n","iteration 1000 / 1500: loss 2.102121\n","iteration 1100 / 1500: loss 2.051949\n","iteration 1200 / 1500: loss 2.074421\n","iteration 1300 / 1500: loss 2.117804\n","iteration 1400 / 1500: loss 2.089864\n","iteration 0 / 1500: loss 860.875551\n","iteration 100 / 1500: loss 105.593596\n","iteration 200 / 1500: loss 14.510887\n","iteration 300 / 1500: loss 3.657454\n","iteration 400 / 1500: loss 2.266871\n","iteration 500 / 1500: loss 2.123714\n","iteration 600 / 1500: loss 2.043589\n","iteration 700 / 1500: loss 2.112829\n","iteration 800 / 1500: loss 2.105090\n","iteration 900 / 1500: loss 2.099378\n","iteration 1000 / 1500: loss 2.085911\n","iteration 1100 / 1500: loss 2.140167\n","iteration 1200 / 1500: loss 2.126590\n","iteration 1300 / 1500: loss 2.109738\n","iteration 1400 / 1500: loss 2.127912\n","iteration 0 / 1500: loss 947.372707\n","iteration 100 / 1500: loss 94.089692\n","iteration 200 / 1500: loss 11.072480\n","iteration 300 / 1500: loss 2.953562\n","iteration 400 / 1500: loss 2.184279\n","iteration 500 / 1500: loss 2.123967\n","iteration 600 / 1500: loss 2.101054\n","iteration 700 / 1500: loss 2.095132\n","iteration 800 / 1500: loss 2.169018\n","iteration 900 / 1500: loss 2.081932\n","iteration 1000 / 1500: loss 2.080569\n","iteration 1100 / 1500: loss 2.133770\n","iteration 1200 / 1500: loss 2.091708\n","iteration 1300 / 1500: loss 2.111045\n","iteration 1400 / 1500: loss 2.074358\n","iteration 0 / 1500: loss 1033.010427\n","iteration 100 / 1500: loss 83.298629\n","iteration 200 / 1500: loss 8.538790\n","iteration 300 / 1500: loss 2.618801\n","iteration 400 / 1500: loss 2.160271\n","iteration 500 / 1500: loss 2.132725\n","iteration 600 / 1500: loss 2.094227\n","iteration 700 / 1500: loss 2.116472\n","iteration 800 / 1500: loss 2.120923\n","iteration 900 / 1500: loss 2.069116\n","iteration 1000 / 1500: loss 2.112023\n","iteration 1100 / 1500: loss 2.159350\n","iteration 1200 / 1500: loss 2.124474\n","iteration 1300 / 1500: loss 2.143575\n","iteration 1400 / 1500: loss 2.112850\n","iteration 0 / 1500: loss 1120.274510\n","iteration 100 / 1500: loss 73.388841\n","iteration 200 / 1500: loss 6.677110\n","iteration 300 / 1500: loss 2.434011\n","iteration 400 / 1500: loss 2.115742\n","iteration 500 / 1500: loss 2.101222\n","iteration 600 / 1500: loss 2.189183\n","iteration 700 / 1500: loss 2.115712\n","iteration 800 / 1500: loss 2.107020\n","iteration 900 / 1500: loss 2.086841\n","iteration 1000 / 1500: loss 2.126540\n","iteration 1100 / 1500: loss 2.110961\n","iteration 1200 / 1500: loss 2.056644\n","iteration 1300 / 1500: loss 2.126329\n","iteration 1400 / 1500: loss 2.092946\n","iteration 0 / 1500: loss 1207.320294\n","iteration 100 / 1500: loss 64.088272\n","iteration 200 / 1500: loss 5.329122\n","iteration 300 / 1500: loss 2.297522\n","iteration 400 / 1500: loss 2.099615\n","iteration 500 / 1500: loss 2.085619\n","iteration 600 / 1500: loss 2.079620\n","iteration 700 / 1500: loss 2.171138\n","iteration 800 / 1500: loss 2.088624\n","iteration 900 / 1500: loss 2.070017\n","iteration 1000 / 1500: loss 2.182961\n","iteration 1100 / 1500: loss 2.117960\n","iteration 1200 / 1500: loss 2.146061\n","iteration 1300 / 1500: loss 2.197220\n","iteration 1400 / 1500: loss 2.079094\n","iteration 0 / 1500: loss 1288.305674\n","iteration 100 / 1500: loss 55.543287\n","iteration 200 / 1500: loss 4.330653\n","iteration 300 / 1500: loss 2.251179\n","iteration 400 / 1500: loss 2.142564\n","iteration 500 / 1500: loss 2.135582\n","iteration 600 / 1500: loss 2.086925\n","iteration 700 / 1500: loss 2.136958\n","iteration 800 / 1500: loss 2.135838\n","iteration 900 / 1500: loss 2.141234\n","iteration 1000 / 1500: loss 2.077959\n","iteration 1100 / 1500: loss 2.090573\n","iteration 1200 / 1500: loss 2.109110\n","iteration 1300 / 1500: loss 2.184336\n","iteration 1400 / 1500: loss 2.134596\n","iteration 0 / 1500: loss 1368.837852\n","iteration 100 / 1500: loss 48.117166\n","iteration 200 / 1500: loss 3.712177\n","iteration 300 / 1500: loss 2.192160\n","iteration 400 / 1500: loss 2.169823\n","iteration 500 / 1500: loss 2.120016\n","iteration 600 / 1500: loss 2.093610\n","iteration 700 / 1500: loss 2.084761\n","iteration 800 / 1500: loss 2.154885\n","iteration 900 / 1500: loss 2.145985\n","iteration 1000 / 1500: loss 2.111766\n","iteration 1100 / 1500: loss 2.075614\n","iteration 1200 / 1500: loss 2.153273\n","iteration 1300 / 1500: loss 2.178806\n","iteration 1400 / 1500: loss 2.125067\n","iteration 0 / 1500: loss 1456.268720\n","iteration 100 / 1500: loss 41.547342\n","iteration 200 / 1500: loss 3.164602\n","iteration 300 / 1500: loss 2.171070\n","iteration 400 / 1500: loss 2.148470\n","iteration 500 / 1500: loss 2.116678\n","iteration 600 / 1500: loss 2.130436\n","iteration 700 / 1500: loss 2.134102\n","iteration 800 / 1500: loss 2.150737\n","iteration 900 / 1500: loss 2.135722\n","iteration 1000 / 1500: loss 2.114974\n","iteration 1100 / 1500: loss 2.087215\n","iteration 1200 / 1500: loss 2.135441\n","iteration 1300 / 1500: loss 2.130160\n","iteration 1400 / 1500: loss 2.145618\n","iteration 0 / 1500: loss 1547.137666\n","iteration 100 / 1500: loss 36.007656\n","iteration 200 / 1500: loss 2.916597\n","iteration 300 / 1500: loss 2.205081\n","iteration 400 / 1500: loss 2.126734\n","iteration 500 / 1500: loss 2.184756\n","iteration 600 / 1500: loss 2.182778\n","iteration 700 / 1500: loss 2.122590\n","iteration 800 / 1500: loss 2.169885\n","iteration 900 / 1500: loss 2.106269\n","iteration 1000 / 1500: loss 2.149608\n","iteration 1100 / 1500: loss 2.122747\n","iteration 1200 / 1500: loss 2.154571\n","iteration 1300 / 1500: loss 2.199017\n","iteration 1400 / 1500: loss 2.153779\n","iteration 0 / 1500: loss 776.227407\n","iteration 100 / 1500: loss 75.515067\n","iteration 200 / 1500: loss 9.094012\n","iteration 300 / 1500: loss 2.745146\n","iteration 400 / 1500: loss 2.119288\n","iteration 500 / 1500: loss 2.019089\n","iteration 600 / 1500: loss 2.084002\n","iteration 700 / 1500: loss 2.108447\n","iteration 800 / 1500: loss 2.100686\n","iteration 900 / 1500: loss 2.039429\n","iteration 1000 / 1500: loss 2.087860\n","iteration 1100 / 1500: loss 2.148136\n","iteration 1200 / 1500: loss 2.076585\n","iteration 1300 / 1500: loss 2.092173\n","iteration 1400 / 1500: loss 2.014136\n","iteration 0 / 1500: loss 870.046194\n","iteration 100 / 1500: loss 65.305273\n","iteration 200 / 1500: loss 6.722950\n","iteration 300 / 1500: loss 2.484037\n","iteration 400 / 1500: loss 2.161160\n","iteration 500 / 1500: loss 2.081364\n","iteration 600 / 1500: loss 2.165352\n","iteration 700 / 1500: loss 2.110858\n","iteration 800 / 1500: loss 2.103460\n","iteration 900 / 1500: loss 2.108077\n","iteration 1000 / 1500: loss 2.074870\n","iteration 1100 / 1500: loss 2.090077\n","iteration 1200 / 1500: loss 2.146671\n","iteration 1300 / 1500: loss 2.099453\n","iteration 1400 / 1500: loss 2.074992\n","iteration 0 / 1500: loss 957.387176\n","iteration 100 / 1500: loss 55.739260\n","iteration 200 / 1500: loss 5.151881\n","iteration 300 / 1500: loss 2.247295\n","iteration 400 / 1500: loss 2.111306\n","iteration 500 / 1500: loss 2.119881\n","iteration 600 / 1500: loss 2.140047\n","iteration 700 / 1500: loss 2.159611\n","iteration 800 / 1500: loss 2.155640\n","iteration 900 / 1500: loss 2.137413\n","iteration 1000 / 1500: loss 2.104945\n","iteration 1100 / 1500: loss 2.121341\n","iteration 1200 / 1500: loss 2.134566\n","iteration 1300 / 1500: loss 2.100209\n","iteration 1400 / 1500: loss 2.079131\n","iteration 0 / 1500: loss 1019.247522\n","iteration 100 / 1500: loss 45.939611\n","iteration 200 / 1500: loss 4.033034\n","iteration 300 / 1500: loss 2.206110\n","iteration 400 / 1500: loss 2.143243\n","iteration 500 / 1500: loss 2.154290\n","iteration 600 / 1500: loss 2.115949\n","iteration 700 / 1500: loss 2.121434\n","iteration 800 / 1500: loss 2.138374\n","iteration 900 / 1500: loss 2.191101\n","iteration 1000 / 1500: loss 2.145168\n","iteration 1100 / 1500: loss 2.107661\n","iteration 1200 / 1500: loss 2.099254\n","iteration 1300 / 1500: loss 2.168021\n","iteration 1400 / 1500: loss 2.125583\n","iteration 0 / 1500: loss 1113.452054\n","iteration 100 / 1500: loss 38.978115\n","iteration 200 / 1500: loss 3.335092\n","iteration 300 / 1500: loss 2.158681\n","iteration 400 / 1500: loss 2.051258\n","iteration 500 / 1500: loss 2.119242\n","iteration 600 / 1500: loss 2.152364\n","iteration 700 / 1500: loss 2.113071\n","iteration 800 / 1500: loss 2.167464\n","iteration 900 / 1500: loss 2.090298\n","iteration 1000 / 1500: loss 2.059480\n","iteration 1100 / 1500: loss 2.128880\n","iteration 1200 / 1500: loss 2.111201\n","iteration 1300 / 1500: loss 2.079211\n","iteration 1400 / 1500: loss 2.121205\n","iteration 0 / 1500: loss 1211.431735\n","iteration 100 / 1500: loss 32.913085\n","iteration 200 / 1500: loss 2.969099\n","iteration 300 / 1500: loss 2.156476\n","iteration 400 / 1500: loss 2.092775\n","iteration 500 / 1500: loss 2.122314\n","iteration 600 / 1500: loss 2.121479\n","iteration 700 / 1500: loss 2.062436\n","iteration 800 / 1500: loss 2.124284\n","iteration 900 / 1500: loss 2.101345\n","iteration 1000 / 1500: loss 2.145294\n","iteration 1100 / 1500: loss 2.154282\n","iteration 1200 / 1500: loss 2.185119\n","iteration 1300 / 1500: loss 2.154416\n","iteration 1400 / 1500: loss 2.145780\n","iteration 0 / 1500: loss 1285.549970\n","iteration 100 / 1500: loss 27.189837\n","iteration 200 / 1500: loss 2.610560\n","iteration 300 / 1500: loss 2.154091\n","iteration 400 / 1500: loss 2.141185\n","iteration 500 / 1500: loss 2.154772\n","iteration 600 / 1500: loss 2.122884\n","iteration 700 / 1500: loss 2.171314\n","iteration 800 / 1500: loss 2.153471\n","iteration 900 / 1500: loss 2.126136\n","iteration 1000 / 1500: loss 2.132734\n","iteration 1100 / 1500: loss 2.153800\n","iteration 1200 / 1500: loss 2.133573\n","iteration 1300 / 1500: loss 2.152006\n","iteration 1400 / 1500: loss 2.108606\n","iteration 0 / 1500: loss 1374.578583\n","iteration 100 / 1500: loss 22.728899\n","iteration 200 / 1500: loss 2.429370\n","iteration 300 / 1500: loss 2.151483\n","iteration 400 / 1500: loss 2.158021\n","iteration 500 / 1500: loss 2.160784\n","iteration 600 / 1500: loss 2.143224\n","iteration 700 / 1500: loss 2.088352\n","iteration 800 / 1500: loss 2.100258\n","iteration 900 / 1500: loss 2.175984\n","iteration 1000 / 1500: loss 2.152657\n","iteration 1100 / 1500: loss 2.123900\n","iteration 1200 / 1500: loss 2.118788\n","iteration 1300 / 1500: loss 2.132684\n","iteration 1400 / 1500: loss 2.100262\n","iteration 0 / 1500: loss 1466.323395\n","iteration 100 / 1500: loss 19.034328\n","iteration 200 / 1500: loss 2.355801\n","iteration 300 / 1500: loss 2.163684\n","iteration 400 / 1500: loss 2.158125\n","iteration 500 / 1500: loss 2.130982\n","iteration 600 / 1500: loss 2.132385\n","iteration 700 / 1500: loss 2.179072\n","iteration 800 / 1500: loss 2.143717\n","iteration 900 / 1500: loss 2.142718\n","iteration 1000 / 1500: loss 2.151476\n","iteration 1100 / 1500: loss 2.161081\n","iteration 1200 / 1500: loss 2.142045\n","iteration 1300 / 1500: loss 2.098960\n","iteration 1400 / 1500: loss 2.169331\n","iteration 0 / 1500: loss 1524.286223\n","iteration 100 / 1500: loss 15.601939\n","iteration 200 / 1500: loss 2.215673\n","iteration 300 / 1500: loss 2.136098\n","iteration 400 / 1500: loss 2.128684\n","iteration 500 / 1500: loss 2.191000\n","iteration 600 / 1500: loss 2.145808\n","iteration 700 / 1500: loss 2.131314\n","iteration 800 / 1500: loss 2.156165\n","iteration 900 / 1500: loss 2.141557\n","iteration 1000 / 1500: loss 2.160093\n","iteration 1100 / 1500: loss 2.159651\n","iteration 1200 / 1500: loss 2.113002\n","iteration 1300 / 1500: loss 2.129243\n","iteration 1400 / 1500: loss 2.164973\n","iteration 0 / 1500: loss 776.189025\n","iteration 100 / 1500: loss 48.865084\n","iteration 200 / 1500: loss 4.938244\n","iteration 300 / 1500: loss 2.304618\n","iteration 400 / 1500: loss 2.092646\n","iteration 500 / 1500: loss 2.059654\n","iteration 600 / 1500: loss 2.089336\n","iteration 700 / 1500: loss 2.120534\n","iteration 800 / 1500: loss 2.059463\n","iteration 900 / 1500: loss 2.116328\n","iteration 1000 / 1500: loss 2.103503\n","iteration 1100 / 1500: loss 2.100366\n","iteration 1200 / 1500: loss 2.061531\n","iteration 1300 / 1500: loss 2.062282\n","iteration 1400 / 1500: loss 2.092186\n","iteration 0 / 1500: loss 857.403106\n","iteration 100 / 1500: loss 39.808033\n","iteration 200 / 1500: loss 3.858538\n","iteration 300 / 1500: loss 2.098463\n","iteration 400 / 1500: loss 2.117839\n","iteration 500 / 1500: loss 2.081336\n","iteration 600 / 1500: loss 2.113399\n","iteration 700 / 1500: loss 2.112978\n","iteration 800 / 1500: loss 2.117867\n","iteration 900 / 1500: loss 2.075916\n","iteration 1000 / 1500: loss 2.062851\n","iteration 1100 / 1500: loss 2.076554\n","iteration 1200 / 1500: loss 2.031506\n","iteration 1300 / 1500: loss 2.116366\n","iteration 1400 / 1500: loss 2.052630\n","iteration 0 / 1500: loss 947.820427\n","iteration 100 / 1500: loss 32.575277\n","iteration 200 / 1500: loss 3.053275\n","iteration 300 / 1500: loss 2.184806\n","iteration 400 / 1500: loss 2.128958\n","iteration 500 / 1500: loss 2.183968\n","iteration 600 / 1500: loss 2.127451\n","iteration 700 / 1500: loss 2.152157\n","iteration 800 / 1500: loss 2.108021\n","iteration 900 / 1500: loss 2.152926\n","iteration 1000 / 1500: loss 2.129702\n","iteration 1100 / 1500: loss 2.155652\n","iteration 1200 / 1500: loss 2.065916\n","iteration 1300 / 1500: loss 2.072784\n","iteration 1400 / 1500: loss 2.093353\n","iteration 0 / 1500: loss 1022.933542\n","iteration 100 / 1500: loss 26.193751\n","iteration 200 / 1500: loss 2.711284\n","iteration 300 / 1500: loss 2.133649\n","iteration 400 / 1500: loss 2.138031\n","iteration 500 / 1500: loss 2.094438\n","iteration 600 / 1500: loss 2.159891\n","iteration 700 / 1500: loss 2.152228\n","iteration 800 / 1500: loss 2.113500\n","iteration 900 / 1500: loss 2.117547\n","iteration 1000 / 1500: loss 2.135092\n","iteration 1100 / 1500: loss 2.094129\n","iteration 1200 / 1500: loss 2.148065\n","iteration 1300 / 1500: loss 2.115143\n","iteration 1400 / 1500: loss 2.131028\n","iteration 0 / 1500: loss 1122.789425\n","iteration 100 / 1500: loss 21.439045\n","iteration 200 / 1500: loss 2.444043\n","iteration 300 / 1500: loss 2.148584\n","iteration 400 / 1500: loss 2.107842\n","iteration 500 / 1500: loss 2.155566\n","iteration 600 / 1500: loss 2.152142\n","iteration 700 / 1500: loss 2.078379\n","iteration 800 / 1500: loss 2.121662\n","iteration 900 / 1500: loss 2.081818\n","iteration 1000 / 1500: loss 2.105689\n","iteration 1100 / 1500: loss 2.078292\n","iteration 1200 / 1500: loss 2.088470\n","iteration 1300 / 1500: loss 2.132152\n","iteration 1400 / 1500: loss 2.111364\n","iteration 0 / 1500: loss 1196.582703\n","iteration 100 / 1500: loss 17.110130\n","iteration 200 / 1500: loss 2.290709\n","iteration 300 / 1500: loss 2.093229\n","iteration 400 / 1500: loss 2.155993\n","iteration 500 / 1500: loss 2.135708\n","iteration 600 / 1500: loss 2.156268\n","iteration 700 / 1500: loss 2.131156\n","iteration 800 / 1500: loss 2.181251\n","iteration 900 / 1500: loss 2.111642\n","iteration 1000 / 1500: loss 2.124975\n","iteration 1100 / 1500: loss 2.137709\n","iteration 1200 / 1500: loss 2.078692\n","iteration 1300 / 1500: loss 2.131542\n","iteration 1400 / 1500: loss 2.143102\n","iteration 0 / 1500: loss 1309.314782\n","iteration 100 / 1500: loss 14.134920\n","iteration 200 / 1500: loss 2.223266\n","iteration 300 / 1500: loss 2.173427\n","iteration 400 / 1500: loss 2.114907\n","iteration 500 / 1500: loss 2.111874\n","iteration 600 / 1500: loss 2.116615\n","iteration 700 / 1500: loss 2.122638\n","iteration 800 / 1500: loss 2.132464\n","iteration 900 / 1500: loss 2.122648\n","iteration 1000 / 1500: loss 2.191103\n","iteration 1100 / 1500: loss 2.118290\n","iteration 1200 / 1500: loss 2.167949\n","iteration 1300 / 1500: loss 2.132771\n","iteration 1400 / 1500: loss 2.119200\n","iteration 0 / 1500: loss 1376.798052\n","iteration 100 / 1500: loss 11.312723\n","iteration 200 / 1500: loss 2.173244\n","iteration 300 / 1500: loss 2.145502\n","iteration 400 / 1500: loss 2.153977\n","iteration 500 / 1500: loss 2.174433\n","iteration 600 / 1500: loss 2.136436\n","iteration 700 / 1500: loss 2.134552\n","iteration 800 / 1500: loss 2.125380\n","iteration 900 / 1500: loss 2.126462\n","iteration 1000 / 1500: loss 2.140554\n","iteration 1100 / 1500: loss 2.168095\n","iteration 1200 / 1500: loss 2.129244\n","iteration 1300 / 1500: loss 2.131214\n","iteration 1400 / 1500: loss 2.062112\n","iteration 0 / 1500: loss 1447.471748\n","iteration 100 / 1500: loss 9.178833\n","iteration 200 / 1500: loss 2.184697\n","iteration 300 / 1500: loss 2.106975\n","iteration 400 / 1500: loss 2.143011\n","iteration 500 / 1500: loss 2.171528\n","iteration 600 / 1500: loss 2.108693\n","iteration 700 / 1500: loss 2.123988\n","iteration 800 / 1500: loss 2.184463\n","iteration 900 / 1500: loss 2.155376\n","iteration 1000 / 1500: loss 2.097889\n","iteration 1100 / 1500: loss 2.125090\n","iteration 1200 / 1500: loss 2.173732\n","iteration 1300 / 1500: loss 2.144210\n","iteration 1400 / 1500: loss 2.133375\n","iteration 0 / 1500: loss 1537.180890\n","iteration 100 / 1500: loss 7.592704\n","iteration 200 / 1500: loss 2.215593\n","iteration 300 / 1500: loss 2.136338\n","iteration 400 / 1500: loss 2.122030\n","iteration 500 / 1500: loss 2.125949\n","iteration 600 / 1500: loss 2.139579\n","iteration 700 / 1500: loss 2.160613\n","iteration 800 / 1500: loss 2.149672\n","iteration 900 / 1500: loss 2.165035\n","iteration 1000 / 1500: loss 2.134805\n","iteration 1100 / 1500: loss 2.079789\n","iteration 1200 / 1500: loss 2.098799\n","iteration 1300 / 1500: loss 2.133071\n","iteration 1400 / 1500: loss 2.137305\n","iteration 0 / 1500: loss 765.298519\n","iteration 100 / 1500: loss 31.473760\n","iteration 200 / 1500: loss 3.230832\n","iteration 300 / 1500: loss 2.140651\n","iteration 400 / 1500: loss 2.062251\n","iteration 500 / 1500: loss 2.083856\n","iteration 600 / 1500: loss 2.131559\n","iteration 700 / 1500: loss 2.055904\n","iteration 800 / 1500: loss 2.046715\n","iteration 900 / 1500: loss 2.069311\n","iteration 1000 / 1500: loss 2.135807\n","iteration 1100 / 1500: loss 2.078997\n","iteration 1200 / 1500: loss 2.100656\n","iteration 1300 / 1500: loss 2.084834\n","iteration 1400 / 1500: loss 2.035271\n","iteration 0 / 1500: loss 869.976063\n","iteration 100 / 1500: loss 25.247478\n","iteration 200 / 1500: loss 2.664982\n","iteration 300 / 1500: loss 2.107523\n","iteration 400 / 1500: loss 2.114744\n","iteration 500 / 1500: loss 2.062373\n","iteration 600 / 1500: loss 2.111099\n","iteration 700 / 1500: loss 2.037737\n","iteration 800 / 1500: loss 2.087628\n","iteration 900 / 1500: loss 2.121206\n","iteration 1000 / 1500: loss 2.124076\n","iteration 1100 / 1500: loss 2.049487\n","iteration 1200 / 1500: loss 2.089532\n","iteration 1300 / 1500: loss 2.122282\n","iteration 1400 / 1500: loss 2.086681\n","iteration 0 / 1500: loss 955.924927\n","iteration 100 / 1500: loss 19.745162\n","iteration 200 / 1500: loss 2.467249\n","iteration 300 / 1500: loss 2.113171\n","iteration 400 / 1500: loss 2.151067\n","iteration 500 / 1500: loss 2.118663\n","iteration 600 / 1500: loss 2.101788\n","iteration 700 / 1500: loss 2.101057\n","iteration 800 / 1500: loss 2.085291\n","iteration 900 / 1500: loss 2.115002\n","iteration 1000 / 1500: loss 2.128072\n","iteration 1100 / 1500: loss 2.092160\n","iteration 1200 / 1500: loss 2.062650\n","iteration 1300 / 1500: loss 2.076807\n","iteration 1400 / 1500: loss 2.133325\n","iteration 0 / 1500: loss 1027.353439\n","iteration 100 / 1500: loss 15.347388\n","iteration 200 / 1500: loss 2.276096\n","iteration 300 / 1500: loss 2.128811\n","iteration 400 / 1500: loss 2.077666\n","iteration 500 / 1500: loss 2.124522\n","iteration 600 / 1500: loss 2.116724\n","iteration 700 / 1500: loss 2.089729\n","iteration 800 / 1500: loss 2.047927\n","iteration 900 / 1500: loss 2.134994\n","iteration 1000 / 1500: loss 2.149858\n","iteration 1100 / 1500: loss 2.119292\n","iteration 1200 / 1500: loss 2.079143\n","iteration 1300 / 1500: loss 2.113917\n","iteration 1400 / 1500: loss 2.129505\n","iteration 0 / 1500: loss 1109.682065\n","iteration 100 / 1500: loss 12.007399\n","iteration 200 / 1500: loss 2.207205\n","iteration 300 / 1500: loss 2.056655\n","iteration 400 / 1500: loss 2.106044\n","iteration 500 / 1500: loss 2.186897\n","iteration 600 / 1500: loss 2.194150\n","iteration 700 / 1500: loss 2.118381\n","iteration 800 / 1500: loss 2.096582\n","iteration 900 / 1500: loss 2.133341\n","iteration 1000 / 1500: loss 2.053122\n","iteration 1100 / 1500: loss 2.154576\n","iteration 1200 / 1500: loss 2.116724\n","iteration 1300 / 1500: loss 2.126689\n","iteration 1400 / 1500: loss 2.120895\n","iteration 0 / 1500: loss 1215.521576\n","iteration 100 / 1500: loss 9.646361\n","iteration 200 / 1500: loss 2.169570\n","iteration 300 / 1500: loss 2.110416\n","iteration 400 / 1500: loss 2.095929\n","iteration 500 / 1500: loss 2.144790\n","iteration 600 / 1500: loss 2.116985\n","iteration 700 / 1500: loss 2.131141\n","iteration 800 / 1500: loss 2.212714\n","iteration 900 / 1500: loss 2.134670\n","iteration 1000 / 1500: loss 2.144704\n","iteration 1100 / 1500: loss 2.141581\n","iteration 1200 / 1500: loss 2.087726\n","iteration 1300 / 1500: loss 2.102729\n","iteration 1400 / 1500: loss 2.133506\n","iteration 0 / 1500: loss 1268.378757\n","iteration 100 / 1500: loss 7.557091\n","iteration 200 / 1500: loss 2.129328\n","iteration 300 / 1500: loss 2.129622\n","iteration 400 / 1500: loss 2.146789\n","iteration 500 / 1500: loss 2.107280\n","iteration 600 / 1500: loss 2.138062\n","iteration 700 / 1500: loss 2.133482\n","iteration 800 / 1500: loss 2.111176\n","iteration 900 / 1500: loss 2.153392\n","iteration 1000 / 1500: loss 2.185977\n","iteration 1100 / 1500: loss 2.105117\n","iteration 1200 / 1500: loss 2.151551\n","iteration 1300 / 1500: loss 2.093228\n","iteration 1400 / 1500: loss 2.171839\n","iteration 0 / 1500: loss 1377.935125\n","iteration 100 / 1500: loss 6.204937\n","iteration 200 / 1500: loss 2.139164\n","iteration 300 / 1500: loss 2.125107\n","iteration 400 / 1500: loss 2.172769\n","iteration 500 / 1500: loss 2.165164\n","iteration 600 / 1500: loss 2.135114\n","iteration 700 / 1500: loss 2.123341\n","iteration 800 / 1500: loss 2.142907\n","iteration 900 / 1500: loss 2.126778\n","iteration 1000 / 1500: loss 2.079855\n","iteration 1100 / 1500: loss 2.111799\n","iteration 1200 / 1500: loss 2.162464\n","iteration 1300 / 1500: loss 2.140328\n","iteration 1400 / 1500: loss 2.137235\n","iteration 0 / 1500: loss 1454.019392\n","iteration 100 / 1500: loss 5.134143\n","iteration 200 / 1500: loss 2.154081\n","iteration 300 / 1500: loss 2.090515\n","iteration 400 / 1500: loss 2.118396\n","iteration 500 / 1500: loss 2.180246\n","iteration 600 / 1500: loss 2.166496\n","iteration 700 / 1500: loss 2.169202\n","iteration 800 / 1500: loss 2.092304\n","iteration 900 / 1500: loss 2.165358\n","iteration 1000 / 1500: loss 2.149072\n","iteration 1100 / 1500: loss 2.136020\n","iteration 1200 / 1500: loss 2.103747\n","iteration 1300 / 1500: loss 2.154473\n","iteration 1400 / 1500: loss 2.135188\n","iteration 0 / 1500: loss 1555.837287\n","iteration 100 / 1500: loss 4.323464\n","iteration 200 / 1500: loss 2.147990\n","iteration 300 / 1500: loss 2.159835\n","iteration 400 / 1500: loss 2.162283\n","iteration 500 / 1500: loss 2.165246\n","iteration 600 / 1500: loss 2.128761\n","iteration 700 / 1500: loss 2.111265\n","iteration 800 / 1500: loss 2.149281\n","iteration 900 / 1500: loss 2.153002\n","iteration 1000 / 1500: loss 2.163707\n","iteration 1100 / 1500: loss 2.092006\n","iteration 1200 / 1500: loss 2.193016\n","iteration 1300 / 1500: loss 2.174251\n","iteration 1400 / 1500: loss 2.166347\n","iteration 0 / 1500: loss 769.188435\n","iteration 100 / 1500: loss 20.826581\n","iteration 200 / 1500: loss 2.587587\n","iteration 300 / 1500: loss 2.092361\n","iteration 400 / 1500: loss 2.078702\n","iteration 500 / 1500: loss 2.124435\n","iteration 600 / 1500: loss 2.088944\n","iteration 700 / 1500: loss 2.113447\n","iteration 800 / 1500: loss 2.089928\n","iteration 900 / 1500: loss 2.074940\n","iteration 1000 / 1500: loss 2.068521\n","iteration 1100 / 1500: loss 2.084880\n","iteration 1200 / 1500: loss 2.151260\n","iteration 1300 / 1500: loss 2.122903\n","iteration 1400 / 1500: loss 2.130501\n","iteration 0 / 1500: loss 865.108107\n","iteration 100 / 1500: loss 16.001892\n","iteration 200 / 1500: loss 2.334706\n","iteration 300 / 1500: loss 2.109654\n","iteration 400 / 1500: loss 2.067405\n","iteration 500 / 1500: loss 2.086159\n","iteration 600 / 1500: loss 2.071995\n","iteration 700 / 1500: loss 2.079264\n","iteration 800 / 1500: loss 2.092375\n","iteration 900 / 1500: loss 2.099610\n","iteration 1000 / 1500: loss 2.046610\n","iteration 1100 / 1500: loss 2.081114\n","iteration 1200 / 1500: loss 2.058087\n","iteration 1300 / 1500: loss 2.118251\n","iteration 1400 / 1500: loss 2.088127\n","iteration 0 / 1500: loss 922.383822\n","iteration 100 / 1500: loss 11.905878\n","iteration 200 / 1500: loss 2.210146\n","iteration 300 / 1500: loss 2.099634\n","iteration 400 / 1500: loss 2.077331\n","iteration 500 / 1500: loss 2.101572\n","iteration 600 / 1500: loss 2.090446\n","iteration 700 / 1500: loss 2.094961\n","iteration 800 / 1500: loss 2.079690\n","iteration 900 / 1500: loss 2.135345\n","iteration 1000 / 1500: loss 2.151772\n","iteration 1100 / 1500: loss 2.078174\n","iteration 1200 / 1500: loss 2.113800\n","iteration 1300 / 1500: loss 2.120135\n","iteration 1400 / 1500: loss 2.110676\n","iteration 0 / 1500: loss 1025.838367\n","iteration 100 / 1500: loss 9.312791\n","iteration 200 / 1500: loss 2.202647\n","iteration 300 / 1500: loss 2.113728\n","iteration 400 / 1500: loss 2.073440\n","iteration 500 / 1500: loss 2.086708\n","iteration 600 / 1500: loss 2.090364\n","iteration 700 / 1500: loss 2.156474\n","iteration 800 / 1500: loss 2.205983\n","iteration 900 / 1500: loss 2.098647\n","iteration 1000 / 1500: loss 2.126037\n","iteration 1100 / 1500: loss 2.152470\n","iteration 1200 / 1500: loss 2.111666\n","iteration 1300 / 1500: loss 2.060268\n","iteration 1400 / 1500: loss 2.096474\n","iteration 0 / 1500: loss 1116.694521\n","iteration 100 / 1500: loss 7.245804\n","iteration 200 / 1500: loss 2.117083\n","iteration 300 / 1500: loss 2.151000\n","iteration 400 / 1500: loss 2.151909\n","iteration 500 / 1500: loss 2.216660\n","iteration 600 / 1500: loss 2.129283\n","iteration 700 / 1500: loss 2.117187\n","iteration 800 / 1500: loss 2.100369\n","iteration 900 / 1500: loss 2.108812\n","iteration 1000 / 1500: loss 2.146917\n","iteration 1100 / 1500: loss 2.170205\n","iteration 1200 / 1500: loss 2.111839\n","iteration 1300 / 1500: loss 2.157349\n","iteration 1400 / 1500: loss 2.090754\n","iteration 0 / 1500: loss 1182.829204\n","iteration 100 / 1500: loss 5.708971\n","iteration 200 / 1500: loss 2.160639\n","iteration 300 / 1500: loss 2.155783\n","iteration 400 / 1500: loss 2.137469\n","iteration 500 / 1500: loss 2.131947\n","iteration 600 / 1500: loss 2.065300\n","iteration 700 / 1500: loss 2.165297\n","iteration 800 / 1500: loss 2.171977\n","iteration 900 / 1500: loss 2.131782\n","iteration 1000 / 1500: loss 2.065208\n","iteration 1100 / 1500: loss 2.172254\n","iteration 1200 / 1500: loss 2.134344\n","iteration 1300 / 1500: loss 2.143910\n","iteration 1400 / 1500: loss 2.107059\n","iteration 0 / 1500: loss 1280.384439\n","iteration 100 / 1500: loss 4.690007\n","iteration 200 / 1500: loss 2.127179\n","iteration 300 / 1500: loss 2.169868\n","iteration 400 / 1500: loss 2.121992\n","iteration 500 / 1500: loss 2.144427\n","iteration 600 / 1500: loss 2.112124\n","iteration 700 / 1500: loss 2.119765\n","iteration 800 / 1500: loss 2.162322\n","iteration 900 / 1500: loss 2.086209\n","iteration 1000 / 1500: loss 2.130973\n","iteration 1100 / 1500: loss 2.151287\n","iteration 1200 / 1500: loss 2.137905\n","iteration 1300 / 1500: loss 2.137678\n","iteration 1400 / 1500: loss 2.080956\n","iteration 0 / 1500: loss 1366.015390\n","iteration 100 / 1500: loss 3.880583\n","iteration 200 / 1500: loss 2.126301\n","iteration 300 / 1500: loss 2.100777\n","iteration 400 / 1500: loss 2.127250\n","iteration 500 / 1500: loss 2.135593\n","iteration 600 / 1500: loss 2.147942\n","iteration 700 / 1500: loss 2.191531\n","iteration 800 / 1500: loss 2.148887\n","iteration 900 / 1500: loss 2.149186\n","iteration 1000 / 1500: loss 2.136263\n","iteration 1100 / 1500: loss 2.096884\n","iteration 1200 / 1500: loss 2.099223\n","iteration 1300 / 1500: loss 2.099594\n","iteration 1400 / 1500: loss 2.105373\n","iteration 0 / 1500: loss 1458.401695\n","iteration 100 / 1500: loss 3.376370\n","iteration 200 / 1500: loss 2.145577\n","iteration 300 / 1500: loss 2.186274\n","iteration 400 / 1500: loss 2.164122\n","iteration 500 / 1500: loss 2.106203\n","iteration 600 / 1500: loss 2.177514\n","iteration 700 / 1500: loss 2.126742\n","iteration 800 / 1500: loss 2.123744\n","iteration 900 / 1500: loss 2.186453\n","iteration 1000 / 1500: loss 2.158877\n","iteration 1100 / 1500: loss 2.152071\n","iteration 1200 / 1500: loss 2.176737\n","iteration 1300 / 1500: loss 2.166518\n","iteration 1400 / 1500: loss 2.116118\n","iteration 0 / 1500: loss 1562.128802\n","iteration 100 / 1500: loss 3.010349\n","iteration 200 / 1500: loss 2.155098\n","iteration 300 / 1500: loss 2.168170\n","iteration 400 / 1500: loss 2.108352\n","iteration 500 / 1500: loss 2.179487\n","iteration 600 / 1500: loss 2.163552\n","iteration 700 / 1500: loss 2.114418\n","iteration 800 / 1500: loss 2.135235\n","iteration 900 / 1500: loss 2.122529\n","iteration 1000 / 1500: loss 2.150696\n","iteration 1100 / 1500: loss 2.196278\n","iteration 1200 / 1500: loss 2.114929\n","iteration 1300 / 1500: loss 2.186999\n","iteration 1400 / 1500: loss 2.165187\n","iteration 0 / 1500: loss 779.593634\n","iteration 100 / 1500: loss 14.157036\n","iteration 200 / 1500: loss 2.312253\n","iteration 300 / 1500: loss 2.100442\n","iteration 400 / 1500: loss 2.111488\n","iteration 500 / 1500: loss 2.056698\n","iteration 600 / 1500: loss 2.116429\n","iteration 700 / 1500: loss 2.024655\n","iteration 800 / 1500: loss 2.093748\n","iteration 900 / 1500: loss 2.089682\n","iteration 1000 / 1500: loss 2.096187\n","iteration 1100 / 1500: loss 2.112990\n","iteration 1200 / 1500: loss 2.031012\n","iteration 1300 / 1500: loss 2.063848\n","iteration 1400 / 1500: loss 2.039355\n","iteration 0 / 1500: loss 854.945238\n","iteration 100 / 1500: loss 10.376915\n","iteration 200 / 1500: loss 2.144641\n","iteration 300 / 1500: loss 2.068749\n","iteration 400 / 1500: loss 2.135725\n","iteration 500 / 1500: loss 2.092176\n","iteration 600 / 1500: loss 2.114675\n","iteration 700 / 1500: loss 2.107931\n","iteration 800 / 1500: loss 2.107150\n","iteration 900 / 1500: loss 2.043028\n","iteration 1000 / 1500: loss 2.098938\n","iteration 1100 / 1500: loss 2.159071\n","iteration 1200 / 1500: loss 2.084213\n","iteration 1300 / 1500: loss 2.132972\n","iteration 1400 / 1500: loss 2.100858\n","iteration 0 / 1500: loss 951.512277\n","iteration 100 / 1500: loss 7.926021\n","iteration 200 / 1500: loss 2.149179\n","iteration 300 / 1500: loss 2.158149\n","iteration 400 / 1500: loss 2.157692\n","iteration 500 / 1500: loss 2.145526\n","iteration 600 / 1500: loss 2.079130\n","iteration 700 / 1500: loss 2.178545\n","iteration 800 / 1500: loss 2.105279\n","iteration 900 / 1500: loss 2.102782\n","iteration 1000 / 1500: loss 2.137353\n","iteration 1100 / 1500: loss 2.094387\n","iteration 1200 / 1500: loss 2.150497\n","iteration 1300 / 1500: loss 2.108619\n","iteration 1400 / 1500: loss 2.091916\n","iteration 0 / 1500: loss 1040.646187\n","iteration 100 / 1500: loss 6.052829\n","iteration 200 / 1500: loss 2.183725\n","iteration 300 / 1500: loss 2.105023\n","iteration 400 / 1500: loss 2.226292\n","iteration 500 / 1500: loss 2.134553\n","iteration 600 / 1500: loss 2.118355\n","iteration 700 / 1500: loss 2.045638\n","iteration 800 / 1500: loss 2.098614\n","iteration 900 / 1500: loss 2.126680\n","iteration 1000 / 1500: loss 2.142134\n","iteration 1100 / 1500: loss 2.077416\n","iteration 1200 / 1500: loss 2.064507\n","iteration 1300 / 1500: loss 2.142618\n","iteration 1400 / 1500: loss 2.152866\n","iteration 0 / 1500: loss 1104.844274\n","iteration 100 / 1500: loss 4.776716\n","iteration 200 / 1500: loss 2.072127\n","iteration 300 / 1500: loss 2.137960\n","iteration 400 / 1500: loss 2.113902\n","iteration 500 / 1500: loss 2.075749\n","iteration 600 / 1500: loss 2.140378\n","iteration 700 / 1500: loss 2.100986\n","iteration 800 / 1500: loss 2.176945\n","iteration 900 / 1500: loss 2.096344\n","iteration 1000 / 1500: loss 2.130866\n","iteration 1100 / 1500: loss 2.099845\n","iteration 1200 / 1500: loss 2.133683\n","iteration 1300 / 1500: loss 2.079833\n","iteration 1400 / 1500: loss 2.132224\n","iteration 0 / 1500: loss 1198.379767\n","iteration 100 / 1500: loss 3.927863\n","iteration 200 / 1500: loss 2.150459\n","iteration 300 / 1500: loss 2.077997\n","iteration 400 / 1500: loss 2.124558\n","iteration 500 / 1500: loss 2.079231\n","iteration 600 / 1500: loss 2.132207\n","iteration 700 / 1500: loss 2.158644\n","iteration 800 / 1500: loss 2.120458\n","iteration 900 / 1500: loss 2.159087\n","iteration 1000 / 1500: loss 2.197672\n","iteration 1100 / 1500: loss 2.119219\n","iteration 1200 / 1500: loss 2.141788\n","iteration 1300 / 1500: loss 2.148468\n","iteration 1400 / 1500: loss 2.120175\n","iteration 0 / 1500: loss 1289.450349\n","iteration 100 / 1500: loss 3.301628\n","iteration 200 / 1500: loss 2.160122\n","iteration 300 / 1500: loss 2.095498\n","iteration 400 / 1500: loss 2.137354\n","iteration 500 / 1500: loss 2.195943\n","iteration 600 / 1500: loss 2.148250\n","iteration 700 / 1500: loss 2.132086\n","iteration 800 / 1500: loss 2.174392\n","iteration 900 / 1500: loss 2.126250\n","iteration 1000 / 1500: loss 2.141258\n","iteration 1100 / 1500: loss 2.114904\n","iteration 1200 / 1500: loss 2.096582\n","iteration 1300 / 1500: loss 2.153137\n","iteration 1400 / 1500: loss 2.148393\n","iteration 0 / 1500: loss 1357.359200\n","iteration 100 / 1500: loss 2.914957\n","iteration 200 / 1500: loss 2.185545\n","iteration 300 / 1500: loss 2.137636\n","iteration 400 / 1500: loss 2.117894\n","iteration 500 / 1500: loss 2.186770\n","iteration 600 / 1500: loss 2.091577\n","iteration 700 / 1500: loss 2.136802\n","iteration 800 / 1500: loss 2.146502\n","iteration 900 / 1500: loss 2.111607\n","iteration 1000 / 1500: loss 2.129489\n","iteration 1100 / 1500: loss 2.110478\n","iteration 1200 / 1500: loss 2.121346\n","iteration 1300 / 1500: loss 2.169994\n","iteration 1400 / 1500: loss 2.197059\n","iteration 0 / 1500: loss 1463.230592\n","iteration 100 / 1500: loss 2.739518\n","iteration 200 / 1500: loss 2.140287\n","iteration 300 / 1500: loss 2.181790\n","iteration 400 / 1500: loss 2.152718\n","iteration 500 / 1500: loss 2.142396\n","iteration 600 / 1500: loss 2.102706\n","iteration 700 / 1500: loss 2.107018\n","iteration 800 / 1500: loss 2.164358\n","iteration 900 / 1500: loss 2.162082\n","iteration 1000 / 1500: loss 2.145726\n","iteration 1100 / 1500: loss 2.196226\n","iteration 1200 / 1500: loss 2.154050\n","iteration 1300 / 1500: loss 2.122538\n","iteration 1400 / 1500: loss 2.124416\n","iteration 0 / 1500: loss 1553.802736\n","iteration 100 / 1500: loss 2.478623\n","iteration 200 / 1500: loss 2.097083\n","iteration 300 / 1500: loss 2.135650\n","iteration 400 / 1500: loss 2.133014\n","iteration 500 / 1500: loss 2.123144\n","iteration 600 / 1500: loss 2.129073\n","iteration 700 / 1500: loss 2.082711\n","iteration 800 / 1500: loss 2.124018\n","iteration 900 / 1500: loss 2.137416\n","iteration 1000 / 1500: loss 2.134393\n","iteration 1100 / 1500: loss 2.157868\n","iteration 1200 / 1500: loss 2.199329\n","iteration 1300 / 1500: loss 2.173915\n","iteration 1400 / 1500: loss 2.178741\n","iteration 0 / 1500: loss 768.145659\n","iteration 100 / 1500: loss 9.552808\n","iteration 200 / 1500: loss 2.226129\n","iteration 300 / 1500: loss 2.079335\n","iteration 400 / 1500: loss 2.101033\n","iteration 500 / 1500: loss 2.046912\n","iteration 600 / 1500: loss 2.104356\n","iteration 700 / 1500: loss 2.072589\n","iteration 800 / 1500: loss 2.030544\n","iteration 900 / 1500: loss 2.077111\n","iteration 1000 / 1500: loss 2.059322\n","iteration 1100 / 1500: loss 2.066153\n","iteration 1200 / 1500: loss 2.036126\n","iteration 1300 / 1500: loss 2.122180\n","iteration 1400 / 1500: loss 2.092574\n","iteration 0 / 1500: loss 855.537907\n","iteration 100 / 1500: loss 7.064671\n","iteration 200 / 1500: loss 2.105566\n","iteration 300 / 1500: loss 2.175013\n","iteration 400 / 1500: loss 2.064757\n","iteration 500 / 1500: loss 2.114362\n","iteration 600 / 1500: loss 2.099497\n","iteration 700 / 1500: loss 2.106621\n","iteration 800 / 1500: loss 2.074541\n","iteration 900 / 1500: loss 2.128476\n","iteration 1000 / 1500: loss 2.162624\n","iteration 1100 / 1500: loss 2.105684\n","iteration 1200 / 1500: loss 2.084773\n","iteration 1300 / 1500: loss 2.081754\n","iteration 1400 / 1500: loss 2.114666\n","iteration 0 / 1500: loss 942.355314\n","iteration 100 / 1500: loss 5.428228\n","iteration 200 / 1500: loss 2.136773\n","iteration 300 / 1500: loss 2.122075\n","iteration 400 / 1500: loss 2.135612\n","iteration 500 / 1500: loss 2.114533\n","iteration 600 / 1500: loss 2.157252\n","iteration 700 / 1500: loss 2.138850\n","iteration 800 / 1500: loss 2.179029\n","iteration 900 / 1500: loss 2.134504\n","iteration 1000 / 1500: loss 2.102627\n","iteration 1100 / 1500: loss 2.115389\n","iteration 1200 / 1500: loss 2.114040\n","iteration 1300 / 1500: loss 2.098916\n","iteration 1400 / 1500: loss 2.094208\n","iteration 0 / 1500: loss 1030.285627\n","iteration 100 / 1500: loss 4.223623\n","iteration 200 / 1500: loss 2.102127\n","iteration 300 / 1500: loss 2.115110\n","iteration 400 / 1500: loss 2.145774\n","iteration 500 / 1500: loss 2.111147\n","iteration 600 / 1500: loss 2.104455\n","iteration 700 / 1500: loss 2.087945\n","iteration 800 / 1500: loss 2.125345\n","iteration 900 / 1500: loss 2.085636\n","iteration 1000 / 1500: loss 2.086788\n","iteration 1100 / 1500: loss 2.096413\n","iteration 1200 / 1500: loss 2.109961\n","iteration 1300 / 1500: loss 2.113734\n","iteration 1400 / 1500: loss 2.150221\n","iteration 0 / 1500: loss 1117.219221\n","iteration 100 / 1500: loss 3.512564\n","iteration 200 / 1500: loss 2.120972\n","iteration 300 / 1500: loss 2.134290\n","iteration 400 / 1500: loss 2.131392\n","iteration 500 / 1500: loss 2.071578\n","iteration 600 / 1500: loss 2.145136\n","iteration 700 / 1500: loss 2.109817\n","iteration 800 / 1500: loss 2.157125\n","iteration 900 / 1500: loss 2.178485\n","iteration 1000 / 1500: loss 2.119181\n","iteration 1100 / 1500: loss 2.140051\n","iteration 1200 / 1500: loss 2.120018\n","iteration 1300 / 1500: loss 2.200291\n","iteration 1400 / 1500: loss 2.117118\n","iteration 0 / 1500: loss 1203.907200\n","iteration 100 / 1500: loss 2.986576\n","iteration 200 / 1500: loss 2.126741\n","iteration 300 / 1500: loss 2.128207\n","iteration 400 / 1500: loss 2.092724\n","iteration 500 / 1500: loss 2.107768\n","iteration 600 / 1500: loss 2.144472\n","iteration 700 / 1500: loss 2.140096\n","iteration 800 / 1500: loss 2.108562\n","iteration 900 / 1500: loss 2.075578\n","iteration 1000 / 1500: loss 2.154080\n","iteration 1100 / 1500: loss 2.164959\n","iteration 1200 / 1500: loss 2.115479\n","iteration 1300 / 1500: loss 2.078062\n","iteration 1400 / 1500: loss 2.117392\n","iteration 0 / 1500: loss 1301.161376\n","iteration 100 / 1500: loss 2.663427\n","iteration 200 / 1500: loss 2.162838\n","iteration 300 / 1500: loss 2.092012\n","iteration 400 / 1500: loss 2.135284\n","iteration 500 / 1500: loss 2.087373\n","iteration 600 / 1500: loss 2.141978\n","iteration 700 / 1500: loss 2.146959\n","iteration 800 / 1500: loss 2.170763\n","iteration 900 / 1500: loss 2.127932\n","iteration 1000 / 1500: loss 2.137405\n","iteration 1100 / 1500: loss 2.152064\n","iteration 1200 / 1500: loss 2.131751\n","iteration 1300 / 1500: loss 2.157190\n","iteration 1400 / 1500: loss 2.137772\n","iteration 0 / 1500: loss 1363.272249\n","iteration 100 / 1500: loss 2.481093\n","iteration 200 / 1500: loss 2.138662\n","iteration 300 / 1500: loss 2.124715\n","iteration 400 / 1500: loss 2.098351\n","iteration 500 / 1500: loss 2.159685\n","iteration 600 / 1500: loss 2.125918\n","iteration 700 / 1500: loss 2.118210\n","iteration 800 / 1500: loss 2.144446\n","iteration 900 / 1500: loss 2.167149\n","iteration 1000 / 1500: loss 2.142842\n","iteration 1100 / 1500: loss 2.128575\n","iteration 1200 / 1500: loss 2.178404\n","iteration 1300 / 1500: loss 2.117227\n","iteration 1400 / 1500: loss 2.157880\n","iteration 0 / 1500: loss 1442.076237\n","iteration 100 / 1500: loss 2.401815\n","iteration 200 / 1500: loss 2.154059\n","iteration 300 / 1500: loss 2.193778\n","iteration 400 / 1500: loss 2.128025\n","iteration 500 / 1500: loss 2.157646\n","iteration 600 / 1500: loss 2.188231\n","iteration 700 / 1500: loss 2.186859\n","iteration 800 / 1500: loss 2.137291\n","iteration 900 / 1500: loss 2.121361\n","iteration 1000 / 1500: loss 2.151829\n","iteration 1100 / 1500: loss 2.109661\n","iteration 1200 / 1500: loss 2.166575\n","iteration 1300 / 1500: loss 2.175209\n","iteration 1400 / 1500: loss 2.187330\n","iteration 0 / 1500: loss 1528.999560\n","iteration 100 / 1500: loss 2.282256\n","iteration 200 / 1500: loss 2.124506\n","iteration 300 / 1500: loss 2.134510\n","iteration 400 / 1500: loss 2.128238\n","iteration 500 / 1500: loss 2.174538\n","iteration 600 / 1500: loss 2.148817\n","iteration 700 / 1500: loss 2.142147\n","iteration 800 / 1500: loss 2.132355\n","iteration 900 / 1500: loss 2.128836\n","iteration 1000 / 1500: loss 2.151914\n","iteration 1100 / 1500: loss 2.161769\n","iteration 1200 / 1500: loss 2.113436\n","iteration 1300 / 1500: loss 2.119279\n","iteration 1400 / 1500: loss 2.111546\n","iteration 0 / 1500: loss 780.010684\n","iteration 100 / 1500: loss 6.891631\n","iteration 200 / 1500: loss 2.066383\n","iteration 300 / 1500: loss 2.109310\n","iteration 400 / 1500: loss 2.064727\n","iteration 500 / 1500: loss 2.087715\n","iteration 600 / 1500: loss 2.059452\n","iteration 700 / 1500: loss 2.088020\n","iteration 800 / 1500: loss 2.073500\n","iteration 900 / 1500: loss 2.081313\n","iteration 1000 / 1500: loss 2.132166\n","iteration 1100 / 1500: loss 2.047034\n","iteration 1200 / 1500: loss 2.116946\n","iteration 1300 / 1500: loss 2.107654\n","iteration 1400 / 1500: loss 2.049260\n","iteration 0 / 1500: loss 864.408557\n","iteration 100 / 1500: loss 5.168702\n","iteration 200 / 1500: loss 2.196505\n","iteration 300 / 1500: loss 2.109583\n","iteration 400 / 1500: loss 2.061209\n","iteration 500 / 1500: loss 2.107808\n","iteration 600 / 1500: loss 2.104334\n","iteration 700 / 1500: loss 2.119531\n","iteration 800 / 1500: loss 2.129319\n","iteration 900 / 1500: loss 2.055732\n","iteration 1000 / 1500: loss 2.140746\n","iteration 1100 / 1500: loss 2.113055\n","iteration 1200 / 1500: loss 2.100441\n","iteration 1300 / 1500: loss 2.038003\n","iteration 1400 / 1500: loss 2.117584\n","iteration 0 / 1500: loss 951.661937\n","iteration 100 / 1500: loss 3.929395\n","iteration 200 / 1500: loss 2.118150\n","iteration 300 / 1500: loss 2.095424\n","iteration 400 / 1500: loss 2.148097\n","iteration 500 / 1500: loss 2.115534\n","iteration 600 / 1500: loss 2.148540\n","iteration 700 / 1500: loss 2.107506\n","iteration 800 / 1500: loss 2.122402\n","iteration 900 / 1500: loss 2.124132\n","iteration 1000 / 1500: loss 2.171086\n","iteration 1100 / 1500: loss 2.159422\n","iteration 1200 / 1500: loss 2.091021\n","iteration 1300 / 1500: loss 2.051600\n","iteration 1400 / 1500: loss 2.125762\n","iteration 0 / 1500: loss 1044.013882\n","iteration 100 / 1500: loss 3.286838\n","iteration 200 / 1500: loss 2.073406\n","iteration 300 / 1500: loss 2.053388\n","iteration 400 / 1500: loss 2.166306\n","iteration 500 / 1500: loss 2.151027\n","iteration 600 / 1500: loss 2.083232\n","iteration 700 / 1500: loss 2.095301\n","iteration 800 / 1500: loss 2.099989\n","iteration 900 / 1500: loss 2.092883\n","iteration 1000 / 1500: loss 2.113671\n","iteration 1100 / 1500: loss 2.149855\n","iteration 1200 / 1500: loss 2.122084\n","iteration 1300 / 1500: loss 2.142630\n","iteration 1400 / 1500: loss 2.153469\n","iteration 0 / 1500: loss 1117.050414\n","iteration 100 / 1500: loss 2.851195\n","iteration 200 / 1500: loss 2.072726\n","iteration 300 / 1500: loss 2.076384\n","iteration 400 / 1500: loss 2.161202\n","iteration 500 / 1500: loss 2.160168\n","iteration 600 / 1500: loss 2.137591\n","iteration 700 / 1500: loss 2.131871\n","iteration 800 / 1500: loss 2.103184\n","iteration 900 / 1500: loss 2.112412\n","iteration 1000 / 1500: loss 2.143392\n","iteration 1100 / 1500: loss 2.151776\n","iteration 1200 / 1500: loss 2.159995\n","iteration 1300 / 1500: loss 2.126090\n","iteration 1400 / 1500: loss 2.135557\n","iteration 0 / 1500: loss 1218.868890\n","iteration 100 / 1500: loss 2.517310\n","iteration 200 / 1500: loss 2.152584\n","iteration 300 / 1500: loss 2.133566\n","iteration 400 / 1500: loss 2.180122\n","iteration 500 / 1500: loss 2.138327\n","iteration 600 / 1500: loss 2.124234\n","iteration 700 / 1500: loss 2.132318\n","iteration 800 / 1500: loss 2.171643\n","iteration 900 / 1500: loss 2.083012\n","iteration 1000 / 1500: loss 2.105708\n","iteration 1100 / 1500: loss 2.134252\n","iteration 1200 / 1500: loss 2.102863\n","iteration 1300 / 1500: loss 2.144828\n","iteration 1400 / 1500: loss 2.106190\n","iteration 0 / 1500: loss 1279.094395\n","iteration 100 / 1500: loss 2.444975\n","iteration 200 / 1500: loss 2.175619\n","iteration 300 / 1500: loss 2.118326\n","iteration 400 / 1500: loss 2.168570\n","iteration 500 / 1500: loss 2.141233\n","iteration 600 / 1500: loss 2.147332\n","iteration 700 / 1500: loss 2.156101\n","iteration 800 / 1500: loss 2.140370\n","iteration 900 / 1500: loss 2.051356\n","iteration 1000 / 1500: loss 2.111392\n","iteration 1100 / 1500: loss 2.172956\n","iteration 1200 / 1500: loss 2.129485\n","iteration 1300 / 1500: loss 2.142955\n","iteration 1400 / 1500: loss 2.142130\n","iteration 0 / 1500: loss 1379.012311\n","iteration 100 / 1500: loss 2.311699\n","iteration 200 / 1500: loss 2.183673\n","iteration 300 / 1500: loss 2.147985\n","iteration 400 / 1500: loss 2.150788\n","iteration 500 / 1500: loss 2.196807\n","iteration 600 / 1500: loss 2.134043\n","iteration 700 / 1500: loss 2.122839\n","iteration 800 / 1500: loss 2.134377\n","iteration 900 / 1500: loss 2.198405\n","iteration 1000 / 1500: loss 2.144962\n","iteration 1100 / 1500: loss 2.125827\n","iteration 1200 / 1500: loss 2.148285\n","iteration 1300 / 1500: loss 2.090803\n","iteration 1400 / 1500: loss 2.143948\n","iteration 0 / 1500: loss 1459.247098\n","iteration 100 / 1500: loss 2.251888\n","iteration 200 / 1500: loss 2.079968\n","iteration 300 / 1500: loss 2.144185\n","iteration 400 / 1500: loss 2.159040\n","iteration 500 / 1500: loss 2.107954\n","iteration 600 / 1500: loss 2.139883\n","iteration 700 / 1500: loss 2.125014\n","iteration 800 / 1500: loss 2.153312\n","iteration 900 / 1500: loss 2.185405\n","iteration 1000 / 1500: loss 2.137154\n","iteration 1100 / 1500: loss 2.170932\n","iteration 1200 / 1500: loss 2.108747\n","iteration 1300 / 1500: loss 2.144934\n","iteration 1400 / 1500: loss 2.122776\n","iteration 0 / 1500: loss 1547.347118\n","iteration 100 / 1500: loss 2.217033\n","iteration 200 / 1500: loss 2.122546\n","iteration 300 / 1500: loss 2.133403\n","iteration 400 / 1500: loss 2.123988\n","iteration 500 / 1500: loss 2.117204\n","iteration 600 / 1500: loss 2.168192\n","iteration 700 / 1500: loss 2.131062\n","iteration 800 / 1500: loss 2.164810\n","iteration 900 / 1500: loss 2.137873\n","iteration 1000 / 1500: loss 2.225753\n","iteration 1100 / 1500: loss 2.200001\n","iteration 1200 / 1500: loss 2.173473\n","iteration 1300 / 1500: loss 2.218684\n","iteration 1400 / 1500: loss 2.166938\n","lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.330306 val accuracy: 0.343000\n","lr 1.000000e-07 reg 2.777778e+04 train accuracy: 0.325286 val accuracy: 0.334000\n","lr 1.000000e-07 reg 3.055556e+04 train accuracy: 0.326551 val accuracy: 0.341000\n","lr 1.000000e-07 reg 3.333333e+04 train accuracy: 0.321571 val accuracy: 0.333000\n","lr 1.000000e-07 reg 3.611111e+04 train accuracy: 0.308122 val accuracy: 0.326000\n","lr 1.000000e-07 reg 3.888889e+04 train accuracy: 0.309143 val accuracy: 0.328000\n","lr 1.000000e-07 reg 4.166667e+04 train accuracy: 0.312327 val accuracy: 0.316000\n","lr 1.000000e-07 reg 4.444444e+04 train accuracy: 0.304898 val accuracy: 0.320000\n","lr 1.000000e-07 reg 4.722222e+04 train accuracy: 0.308918 val accuracy: 0.326000\n","lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.308347 val accuracy: 0.329000\n","lr 1.444444e-07 reg 2.500000e+04 train accuracy: 0.329429 val accuracy: 0.337000\n","lr 1.444444e-07 reg 2.777778e+04 train accuracy: 0.317898 val accuracy: 0.329000\n","lr 1.444444e-07 reg 3.055556e+04 train accuracy: 0.314878 val accuracy: 0.342000\n","lr 1.444444e-07 reg 3.333333e+04 train accuracy: 0.320000 val accuracy: 0.327000\n","lr 1.444444e-07 reg 3.611111e+04 train accuracy: 0.314571 val accuracy: 0.330000\n","lr 1.444444e-07 reg 3.888889e+04 train accuracy: 0.308918 val accuracy: 0.324000\n","lr 1.444444e-07 reg 4.166667e+04 train accuracy: 0.316122 val accuracy: 0.330000\n","lr 1.444444e-07 reg 4.444444e+04 train accuracy: 0.307796 val accuracy: 0.330000\n","lr 1.444444e-07 reg 4.722222e+04 train accuracy: 0.301571 val accuracy: 0.316000\n","lr 1.444444e-07 reg 5.000000e+04 train accuracy: 0.302857 val accuracy: 0.316000\n","lr 1.888889e-07 reg 2.500000e+04 train accuracy: 0.324061 val accuracy: 0.347000\n","lr 1.888889e-07 reg 2.777778e+04 train accuracy: 0.331755 val accuracy: 0.352000\n","lr 1.888889e-07 reg 3.055556e+04 train accuracy: 0.329939 val accuracy: 0.342000\n","lr 1.888889e-07 reg 3.333333e+04 train accuracy: 0.323939 val accuracy: 0.342000\n","lr 1.888889e-07 reg 3.611111e+04 train accuracy: 0.309224 val accuracy: 0.331000\n","lr 1.888889e-07 reg 3.888889e+04 train accuracy: 0.310429 val accuracy: 0.335000\n","lr 1.888889e-07 reg 4.166667e+04 train accuracy: 0.304714 val accuracy: 0.327000\n","lr 1.888889e-07 reg 4.444444e+04 train accuracy: 0.312347 val accuracy: 0.330000\n","lr 1.888889e-07 reg 4.722222e+04 train accuracy: 0.306571 val accuracy: 0.319000\n","lr 1.888889e-07 reg 5.000000e+04 train accuracy: 0.312592 val accuracy: 0.325000\n","lr 2.333333e-07 reg 2.500000e+04 train accuracy: 0.332082 val accuracy: 0.344000\n","lr 2.333333e-07 reg 2.777778e+04 train accuracy: 0.317980 val accuracy: 0.337000\n","lr 2.333333e-07 reg 3.055556e+04 train accuracy: 0.320000 val accuracy: 0.327000\n","lr 2.333333e-07 reg 3.333333e+04 train accuracy: 0.316082 val accuracy: 0.334000\n","lr 2.333333e-07 reg 3.611111e+04 train accuracy: 0.313265 val accuracy: 0.327000\n","lr 2.333333e-07 reg 3.888889e+04 train accuracy: 0.318224 val accuracy: 0.329000\n","lr 2.333333e-07 reg 4.166667e+04 train accuracy: 0.313735 val accuracy: 0.327000\n","lr 2.333333e-07 reg 4.444444e+04 train accuracy: 0.308755 val accuracy: 0.318000\n","lr 2.333333e-07 reg 4.722222e+04 train accuracy: 0.315898 val accuracy: 0.320000\n","lr 2.333333e-07 reg 5.000000e+04 train accuracy: 0.310796 val accuracy: 0.319000\n","lr 2.777778e-07 reg 2.500000e+04 train accuracy: 0.321980 val accuracy: 0.345000\n","lr 2.777778e-07 reg 2.777778e+04 train accuracy: 0.330490 val accuracy: 0.347000\n","lr 2.777778e-07 reg 3.055556e+04 train accuracy: 0.329878 val accuracy: 0.346000\n","lr 2.777778e-07 reg 3.333333e+04 train accuracy: 0.310531 val accuracy: 0.330000\n","lr 2.777778e-07 reg 3.611111e+04 train accuracy: 0.299980 val accuracy: 0.315000\n","lr 2.777778e-07 reg 3.888889e+04 train accuracy: 0.307959 val accuracy: 0.329000\n","lr 2.777778e-07 reg 4.166667e+04 train accuracy: 0.318020 val accuracy: 0.320000\n","lr 2.777778e-07 reg 4.444444e+04 train accuracy: 0.306510 val accuracy: 0.319000\n","lr 2.777778e-07 reg 4.722222e+04 train accuracy: 0.303020 val accuracy: 0.316000\n","lr 2.777778e-07 reg 5.000000e+04 train accuracy: 0.304469 val accuracy: 0.325000\n","lr 3.222222e-07 reg 2.500000e+04 train accuracy: 0.326776 val accuracy: 0.341000\n","lr 3.222222e-07 reg 2.777778e+04 train accuracy: 0.317347 val accuracy: 0.326000\n","lr 3.222222e-07 reg 3.055556e+04 train accuracy: 0.325571 val accuracy: 0.341000\n","lr 3.222222e-07 reg 3.333333e+04 train accuracy: 0.314878 val accuracy: 0.335000\n","lr 3.222222e-07 reg 3.611111e+04 train accuracy: 0.318510 val accuracy: 0.333000\n","lr 3.222222e-07 reg 3.888889e+04 train accuracy: 0.319204 val accuracy: 0.332000\n","lr 3.222222e-07 reg 4.166667e+04 train accuracy: 0.315041 val accuracy: 0.322000\n","lr 3.222222e-07 reg 4.444444e+04 train accuracy: 0.308224 val accuracy: 0.320000\n","lr 3.222222e-07 reg 4.722222e+04 train accuracy: 0.301184 val accuracy: 0.314000\n","lr 3.222222e-07 reg 5.000000e+04 train accuracy: 0.304980 val accuracy: 0.323000\n","lr 3.666667e-07 reg 2.500000e+04 train accuracy: 0.316020 val accuracy: 0.334000\n","lr 3.666667e-07 reg 2.777778e+04 train accuracy: 0.318735 val accuracy: 0.324000\n","lr 3.666667e-07 reg 3.055556e+04 train accuracy: 0.320551 val accuracy: 0.330000\n","lr 3.666667e-07 reg 3.333333e+04 train accuracy: 0.310714 val accuracy: 0.326000\n","lr 3.666667e-07 reg 3.611111e+04 train accuracy: 0.315367 val accuracy: 0.328000\n","lr 3.666667e-07 reg 3.888889e+04 train accuracy: 0.315673 val accuracy: 0.328000\n","lr 3.666667e-07 reg 4.166667e+04 train accuracy: 0.312347 val accuracy: 0.323000\n","lr 3.666667e-07 reg 4.444444e+04 train accuracy: 0.314673 val accuracy: 0.327000\n","lr 3.666667e-07 reg 4.722222e+04 train accuracy: 0.314857 val accuracy: 0.321000\n","lr 3.666667e-07 reg 5.000000e+04 train accuracy: 0.298122 val accuracy: 0.315000\n","lr 4.111111e-07 reg 2.500000e+04 train accuracy: 0.325408 val accuracy: 0.337000\n","lr 4.111111e-07 reg 2.777778e+04 train accuracy: 0.325980 val accuracy: 0.343000\n","lr 4.111111e-07 reg 3.055556e+04 train accuracy: 0.309939 val accuracy: 0.325000\n","lr 4.111111e-07 reg 3.333333e+04 train accuracy: 0.323143 val accuracy: 0.339000\n","lr 4.111111e-07 reg 3.611111e+04 train accuracy: 0.312694 val accuracy: 0.334000\n","lr 4.111111e-07 reg 3.888889e+04 train accuracy: 0.311898 val accuracy: 0.316000\n","lr 4.111111e-07 reg 4.166667e+04 train accuracy: 0.322204 val accuracy: 0.319000\n","lr 4.111111e-07 reg 4.444444e+04 train accuracy: 0.317082 val accuracy: 0.338000\n","lr 4.111111e-07 reg 4.722222e+04 train accuracy: 0.290143 val accuracy: 0.304000\n","lr 4.111111e-07 reg 5.000000e+04 train accuracy: 0.306510 val accuracy: 0.316000\n","lr 4.555556e-07 reg 2.500000e+04 train accuracy: 0.321429 val accuracy: 0.334000\n","lr 4.555556e-07 reg 2.777778e+04 train accuracy: 0.312673 val accuracy: 0.340000\n","lr 4.555556e-07 reg 3.055556e+04 train accuracy: 0.322633 val accuracy: 0.345000\n","lr 4.555556e-07 reg 3.333333e+04 train accuracy: 0.321633 val accuracy: 0.333000\n","lr 4.555556e-07 reg 3.611111e+04 train accuracy: 0.310878 val accuracy: 0.323000\n","lr 4.555556e-07 reg 3.888889e+04 train accuracy: 0.303857 val accuracy: 0.318000\n","lr 4.555556e-07 reg 4.166667e+04 train accuracy: 0.298184 val accuracy: 0.307000\n","lr 4.555556e-07 reg 4.444444e+04 train accuracy: 0.308755 val accuracy: 0.316000\n","lr 4.555556e-07 reg 4.722222e+04 train accuracy: 0.297306 val accuracy: 0.320000\n","lr 4.555556e-07 reg 5.000000e+04 train accuracy: 0.311286 val accuracy: 0.317000\n","lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.319245 val accuracy: 0.342000\n","lr 5.000000e-07 reg 2.777778e+04 train accuracy: 0.324857 val accuracy: 0.328000\n","lr 5.000000e-07 reg 3.055556e+04 train accuracy: 0.308551 val accuracy: 0.325000\n","lr 5.000000e-07 reg 3.333333e+04 train accuracy: 0.320653 val accuracy: 0.340000\n","lr 5.000000e-07 reg 3.611111e+04 train accuracy: 0.314020 val accuracy: 0.322000\n","lr 5.000000e-07 reg 3.888889e+04 train accuracy: 0.310245 val accuracy: 0.319000\n","lr 5.000000e-07 reg 4.166667e+04 train accuracy: 0.308735 val accuracy: 0.330000\n","lr 5.000000e-07 reg 4.444444e+04 train accuracy: 0.301898 val accuracy: 0.312000\n","lr 5.000000e-07 reg 4.722222e+04 train accuracy: 0.312388 val accuracy: 0.323000\n","lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.305837 val accuracy: 0.316000\n","best validation accuracy achieved during cross-validation: 0.352000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"test","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595477922751,"user_tz":240,"elapsed":187,"user":{"displayName":"Emerson Li","photoUrl":"","userId":"16602955780450158924"}},"outputId":"d7a7efa7-ef58-4fa3-8078-347f66f4612e"},"source":["# evaluate on test set\n","# Evaluate the best softmax on test set\n","y_test_pred = best_softmax.predict(X_test)\n","test_accuracy = np.mean(y_test == y_test_pred)\n","print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"],"execution_count":84,"outputs":[{"output_type":"stream","text":["softmax on raw pixels final test set accuracy: 0.349000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"tags":["pdf-inline"],"id":"3XFzh0JW00c7","colab_type":"text"},"source":["**Inline Question 2** - *True or False*\n","\n","Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n","\n","$\\color{blue}{\\textit Your Answer:}$ True\n","\n","\n","$\\color{blue}{\\textit Your Explanation:}$ If the margin for that point's svm output for all classes's less than zero, thus is bounded at zero, then the svm will be unchanged.\n","\n"]},{"cell_type":"code","metadata":{"id":"kweVRQJj00c8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"status":"ok","timestamp":1595477981924,"user_tz":240,"elapsed":714,"user":{"displayName":"Emerson Li","photoUrl":"","userId":"16602955780450158924"}},"outputId":"eedc6a66-ee00-4056-cebd-71fd1d2d8bcd"},"source":["# Visualize the learned weights for each class\n","w = best_softmax.W[:-1,:] # strip out the bias\n","w = w.reshape(32, 32, 3, 10)\n","\n","w_min, w_max = np.min(w), np.max(w)\n","\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","for i in range(10):\n","    plt.subplot(2, 5, i + 1)\n","    \n","    # Rescale the weights to be between 0 and 255\n","    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n","    plt.imshow(wimg.astype('uint8'))\n","    plt.axis('off')\n","    plt.title(classes[i])"],"execution_count":85,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x576 with 10 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"viNLcj2600c-","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hr88_Olv00c_","colab_type":"text"},"source":["---\n","# IMPORTANT\n","\n","This is the end of this question. Please do the following:\n","\n","1. Click `File -> Save` to make sure the latest checkpoint of this notebook is saved to your Drive.\n","2. Execute the cell below to download the modified `.py` files back to your drive."]},{"cell_type":"code","metadata":{"id":"aUzj6HEU00dA","colab_type":"code","colab":{}},"source":["import os\n","\n","FOLDER_TO_SAVE = os.path.join('drive/My Drive/', FOLDERNAME)\n","FILES_TO_SAVE = ['cs231n/classifiers/softmax.py']\n","\n","for files in FILES_TO_SAVE:\n","  with open(os.path.join(FOLDER_TO_SAVE, '/'.join(files.split('/')[1:])), 'w') as f:\n","    f.write(''.join(open(files).readlines()))"],"execution_count":null,"outputs":[]}]}